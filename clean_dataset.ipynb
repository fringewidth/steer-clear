{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf203844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>human1\\nDude, do you ever think we're living in a simulation?\\n<|im_end|>\\n\\n<|im_start|>human2\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"scenarios_cleaned.csv\")\n",
    "df['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenarios_model import scenarios_json\n",
    "\n",
    "# Create mapping\n",
    "template_map = {s['name']: s['context'] for s in scenarios_json}\n",
    "\n",
    "# Map and concatenate\n",
    "df['full_prompt'] = df['category'].map(template_map) + \"\\n\" + df['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map={\"\": \"cuda:0\"})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9c0bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = df['full_prompt'].tolist()\n",
    "\n",
    "all_outputs = []\n",
    "batch_size = len(df)\n",
    "\n",
    "for i in range(0, len(prompts), batch_size):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    # decode all escape sequences like \\n, \\t, \\; etc.\n",
    "    batch_prompts = [bytes(p, \"utf-8\").decode(\"unicode_escape\") for p in batch_prompts]\n",
    "    \n",
    "    inputs = tokenizer(batch_prompts, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    all_outputs.extend(decoded)\n",
    "\n",
    "df['original_completion'] = all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51b93e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scenarios_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc562c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       category  \\\n",
      "0     Stoned Roommates Chilling   \n",
      "1     Stoned Roommates Chilling   \n",
      "2     Stoned Roommates Chilling   \n",
      "3     Stoned Roommates Chilling   \n",
      "4     Stoned Roommates Chilling   \n",
      "..                          ...   \n",
      "475  Commuters on Delayed Train   \n",
      "476  Commuters on Delayed Train   \n",
      "477  Commuters on Delayed Train   \n",
      "478  Commuters on Delayed Train   \n",
      "479  Commuters on Delayed Train   \n",
      "\n",
      "                                                prompt  \\\n",
      "0    <|im_start|>human1\\nDude, do you ever think we...   \n",
      "1    <|im_start|>human1\\nOkay, but seriously, what'...   \n",
      "2    <|im_start|>human1\\nIf you could have any supe...   \n",
      "3    <|im_start|>human1\\nWhat's a conspiracy theory...   \n",
      "4    <|im_start|>human1\\nIf you could only eat one ...   \n",
      "..                                                 ...   \n",
      "475  <|im_start|>commuter1\\n\"I'm starting to think ...   \n",
      "476  <|im_start|>commuter1\\n\"What's the most ridicu...   \n",
      "477  <|im_start|>commuter1\\n\"I'm convinced they del...   \n",
      "478  <|im_start|>commuter1\\n\"Does anyone else feel ...   \n",
      "479  <|im_start|>commuter1\\n\"Honestly, I'd rather b...   \n",
      "\n",
      "                                           full_prompt  \\\n",
      "0    Two college roommates who are high, relaxed, a...   \n",
      "1    Two college roommates who are high, relaxed, a...   \n",
      "2    Two college roommates who are high, relaxed, a...   \n",
      "3    Two college roommates who are high, relaxed, a...   \n",
      "4    Two college roommates who are high, relaxed, a...   \n",
      "..                                                 ...   \n",
      "475  Strangers on a delayed train, shared frustrati...   \n",
      "476  Strangers on a delayed train, shared frustrati...   \n",
      "477  Strangers on a delayed train, shared frustrati...   \n",
      "478  Strangers on a delayed train, shared frustrati...   \n",
      "479  Strangers on a delayed train, shared frustrati...   \n",
      "\n",
      "                                   original_completion  \n",
      "0    Two college roommates who are high, relaxed, a...  \n",
      "1    Two college roommates who are high, relaxed, a...  \n",
      "2    Two college roommates who are high, relaxed, a...  \n",
      "3    Two college roommates who are high, relaxed, a...  \n",
      "4    Two college roommates who are high, relaxed, a...  \n",
      "..                                                 ...  \n",
      "475  Strangers on a delayed train, shared frustrati...  \n",
      "476  Strangers on a delayed train, shared frustrati...  \n",
      "477  Strangers on a delayed train, shared frustrati...  \n",
      "478  Strangers on a delayed train, shared frustrati...  \n",
      "479  Strangers on a delayed train, shared frustrati...  \n",
      "\n",
      "[480 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
