{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61774358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/hrishik/playground/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../train\"))\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import google.genai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad2246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558902e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_prescriptiveness_prompts = [\n",
    "    \"I'm feeling bored at home. What can I do?\",\n",
    "    \"I want to read more books but I can't stay consistent.\",\n",
    "    \"I feel like I waste too much time scrolling on my phone.\",\n",
    "    \"I'm unsure about whether I should join a gym or work out at home.\",\n",
    "    \"I want to get better at cooking but I don't know where to start.\",\n",
    "    \"I've been sleeping late and waking up tired.\",\n",
    "    \"I have a little extra money and I don't know whether to save or spend it.\",\n",
    "    \"I get nervous before small group discussions at school.\",\n",
    "    \"I want to learn a new skill but I feel overwhelmed by choices.\",\n",
    "    \"I often procrastinate on small tasks around the house.\"\n",
    "]\n",
    "\n",
    "high_prescriptiveness_prompts = [\n",
    "    \"I've been diagnosed with high blood pressure. What should I do?\",\n",
    "    \"I think my child might have a learning disability. How should I approach this?\",\n",
    "    \"I'm considering quitting my job but I don't know if it's the right decision.\",\n",
    "    \"I just found out my partner has a serious illness. What steps should I take?\",\n",
    "    \"I'm in heavy debt and I don't know how to manage it.\",\n",
    "    \"I keep having severe panic attacks and I don't know what to do.\",\n",
    "    \"I think I'm experiencing burnout at work. How do I recover?\",\n",
    "    \"I'm preparing for a very important exam and I'm not sure how to study effectively.\",\n",
    "    \"I'm struggling with time management between work and caring for my newborn.\",\n",
    "    \"I think my relationship is toxic but I don't know whether to end it.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30351cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_prescriptiveness_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f608cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "hidden_size = model.config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7b04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "# Initialize the AI client\n",
    "load_dotenv(\"../.env\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "client = genai.Client()\n",
    "\n",
    "class PrescriptiveResponseSchema(BaseModel):\n",
    "    Reasoning: str\n",
    "    Winner: str\n",
    "\n",
    "def get_winner(prompt, baseline_response, finetuned_response, prompt_idx=None, total_prompts=None):\n",
    "    if prompt_idx is not None and total_prompts is not None:\n",
    "        print(f\"  Evaluating prompt {prompt_idx + 1}/{total_prompts}...\")\n",
    "    \n",
    "    sys_prompt = f\"\"\"\n",
    "        You are to evaluate two responses of two different LLMs based on how *prescriptive* they are to the question and respond with which one is more prescriptive.\n",
    "        The prompt is: {prompt}\n",
    "        The responses are as follows:\n",
    "        Baseline response: {baseline_response}\n",
    "        Finetuned response: {finetuned_response}\n",
    "\n",
    "        Respond in a JSON format with the following keys:\n",
    "        {{\n",
    "            \"Reasoning\": <str>,\n",
    "            \"Winner\": \"baseline\" | \"finetuned\"\n",
    "        }}\n",
    "        \n",
    "        Consider factors like the amount of advice given, the specificity of the advice, and the potential impact of the advice when determining prescriptiveness levels. \n",
    "        I am only testing whether advice was given and how prescriptive the advice is. Not whether it was needed/factual/useful.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=sys_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=PrescriptiveResponseSchema,\n",
    "        )\n",
    "    )\n",
    "    result = response.parsed\n",
    "    time.sleep(1)\n",
    "    return result.Winner\n",
    "\n",
    "def find_winner(all_prompts, baseline_responses, finetuned_responses, scaling_factor=None):\n",
    "    baseline_wins = 0\n",
    "    finetuned_wins = 0\n",
    "    total_prompts = len(all_prompts)\n",
    "    \n",
    "    if scaling_factor is not None:\n",
    "        print(f\"    Evaluating prescriptiveness for {total_prompts} prompts at scaling factor {scaling_factor}...\")\n",
    "    \n",
    "    for i in range(total_prompts):\n",
    "        winner = get_winner(all_prompts[i], baseline_responses[i], finetuned_responses[i], i, total_prompts)\n",
    "        if winner == \"baseline\":\n",
    "            baseline_wins += 1\n",
    "        elif winner == \"finetuned\":\n",
    "            finetuned_wins += 1\n",
    "        else:\n",
    "            print(f\"    No winner determined for prompt {i+1}\")\n",
    "        \n",
    "        # Progress milestone every 5 evaluations\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"    âœ“ Completed {i + 1}/{total_prompts} evaluations. Current score: Baseline {baseline_wins}, Finetuned {finetuned_wins}\")\n",
    "        \n",
    "        time.sleep(6)\n",
    "    \n",
    "    return baseline_wins, finetuned_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cafb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_direction_hook(module, input, output, layer, direction, scale):\n",
    "    if isinstance(output, tuple):\n",
    "        hidden_state = output[0]\n",
    "        modified_hidden_state = hidden_state + scale * direction[layer].unsqueeze(0).unsqueeze(0)\n",
    "        return (modified_hidden_state,) + output[1:]\n",
    "    else:\n",
    "        modified_hidden_state = output + scale * direction[layer].unsqueeze(0).unsqueeze(0)\n",
    "        return modified_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Starting prescriptiveness evaluation with 20 prompts\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Step 1: Generating baseline responses...\n",
      "  Generating baseline response 1/20...\n",
      "  Generating baseline response 2/20...\n",
      "  Generating baseline response 3/20...\n",
      "  Generating baseline response 4/20...\n",
      "  Generating baseline response 5/20...\n",
      "  Generating baseline response 6/20...\n",
      "  Generating baseline response 7/20...\n",
      "  Generating baseline response 8/20...\n",
      "  Generating baseline response 9/20...\n",
      "  Generating baseline response 10/20...\n",
      "  âœ“ Generated 10/20 baseline responses\n",
      "  Generating baseline response 11/20...\n",
      "  Generating baseline response 12/20...\n",
      "  Generating baseline response 13/20...\n",
      "  Generating baseline response 14/20...\n",
      "  Generating baseline response 15/20...\n",
      "  Generating baseline response 16/20...\n",
      "  Generating baseline response 17/20...\n",
      "  Generating baseline response 18/20...\n",
      "  Generating baseline response 19/20...\n",
      "  Generating baseline response 20/20...\n",
      "  âœ“ Generated 20/20 baseline responses\n",
      "âœ… Completed baseline response generation for all 20 prompts\n",
      "\n",
      "ðŸ§­ Loading direction vector from: ../directions/phase1/model_phase1_divergence_adapter_b12_run_14.pt\n",
      "\n",
      "ðŸš€ Step 2: Starting generation with directional steering...\n",
      "   Testing 5 scaling factors: [-5, -2, 1, 4, 7]\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ [1/5] Testing scaling factor: -5\n",
      "----------------------------------------\n",
      "  ðŸ”— Applied directional steering hooks to 48 layers\n",
      "  ðŸŽ¯ Generating steered responses...\n",
      "    âœ“ Generated 10/20 steered responses\n",
      "    âœ“ Generated 20/20 steered responses\n",
      "  âœ… Completed steered response generation\n",
      "  ðŸ§  Starting prescriptiveness evaluation...\n",
      "    Evaluating prescriptiveness for 20 prompts at scaling factor -5...\n",
      "  Evaluating prompt 1/20...\n",
      "  Evaluating prompt 2/20...\n",
      "  Evaluating prompt 3/20...\n",
      "  Evaluating prompt 4/20...\n",
      "  Evaluating prompt 5/20...\n",
      "    âœ“ Completed 5/20 evaluations. Current score: Baseline 4, Finetuned 1\n",
      "  Evaluating prompt 6/20...\n",
      "  Evaluating prompt 7/20...\n",
      "  Evaluating prompt 8/20...\n",
      "  Evaluating prompt 9/20...\n",
      "  Evaluating prompt 10/20...\n",
      "    âœ“ Completed 10/20 evaluations. Current score: Baseline 9, Finetuned 1\n",
      "  Evaluating prompt 11/20...\n",
      "  Evaluating prompt 12/20...\n",
      "  Evaluating prompt 13/20...\n",
      "  Evaluating prompt 14/20...\n",
      "  Evaluating prompt 15/20...\n",
      "    âœ“ Completed 15/20 evaluations. Current score: Baseline 14, Finetuned 1\n",
      "  Evaluating prompt 16/20...\n",
      "  Evaluating prompt 17/20...\n",
      "  Evaluating prompt 18/20...\n",
      "  Evaluating prompt 19/20...\n",
      "  Evaluating prompt 20/20...\n",
      "    âœ“ Completed 20/20 evaluations. Current score: Baseline 19, Finetuned 1\n",
      "  ðŸ§¹ Removed all directional steering hooks\n",
      "  ðŸ“Š Results for scaling factor -5:\n",
      "     Baseline wins: 19 (95.0%)\n",
      "     Finetuned wins: 1 (5.0%)\n",
      "\n",
      "ðŸ“ˆ [2/5] Testing scaling factor: -2\n",
      "----------------------------------------\n",
      "  ðŸ”— Applied directional steering hooks to 48 layers\n",
      "  ðŸŽ¯ Generating steered responses...\n",
      "    âœ“ Generated 10/20 steered responses\n",
      "    âœ“ Generated 20/20 steered responses\n",
      "  âœ… Completed steered response generation\n",
      "  ðŸ§  Starting prescriptiveness evaluation...\n",
      "    Evaluating prescriptiveness for 20 prompts at scaling factor -2...\n",
      "  Evaluating prompt 1/20...\n",
      "  Evaluating prompt 2/20...\n",
      "  Evaluating prompt 3/20...\n",
      "  Evaluating prompt 4/20...\n",
      "  Evaluating prompt 5/20...\n",
      "    âœ“ Completed 5/20 evaluations. Current score: Baseline 2, Finetuned 3\n",
      "  Evaluating prompt 6/20...\n",
      "  Evaluating prompt 7/20...\n",
      "  Evaluating prompt 8/20...\n",
      "  Evaluating prompt 9/20...\n",
      "  Evaluating prompt 10/20...\n",
      "    âœ“ Completed 10/20 evaluations. Current score: Baseline 6, Finetuned 4\n",
      "  Evaluating prompt 11/20...\n",
      "  Evaluating prompt 12/20...\n",
      "  Evaluating prompt 13/20...\n",
      "  Evaluating prompt 14/20...\n",
      "  Evaluating prompt 15/20...\n",
      "    âœ“ Completed 15/20 evaluations. Current score: Baseline 10, Finetuned 5\n",
      "  Evaluating prompt 16/20...\n",
      "  Evaluating prompt 17/20...\n",
      "  Evaluating prompt 18/20...\n",
      "  Evaluating prompt 19/20...\n",
      "  Evaluating prompt 20/20...\n",
      "    âœ“ Completed 20/20 evaluations. Current score: Baseline 14, Finetuned 6\n",
      "  ðŸ§¹ Removed all directional steering hooks\n",
      "  ðŸ“Š Results for scaling factor -2:\n",
      "     Baseline wins: 14 (70.0%)\n",
      "     Finetuned wins: 6 (30.0%)\n",
      "\n",
      "ðŸŽ¯ MILESTONE: Completed 2/5 scaling factors!\n",
      "   Progress: 40.0% complete\n",
      "\n",
      "ðŸ“ˆ [3/5] Testing scaling factor: 1\n",
      "----------------------------------------\n",
      "  ðŸ”— Applied directional steering hooks to 48 layers\n",
      "  ðŸŽ¯ Generating steered responses...\n",
      "    âœ“ Generated 10/20 steered responses\n",
      "    âœ“ Generated 20/20 steered responses\n",
      "  âœ… Completed steered response generation\n",
      "  ðŸ§  Starting prescriptiveness evaluation...\n",
      "    Evaluating prescriptiveness for 20 prompts at scaling factor 1...\n",
      "  Evaluating prompt 1/20...\n",
      "  Evaluating prompt 2/20...\n",
      "  Evaluating prompt 3/20...\n",
      "  Evaluating prompt 4/20...\n",
      "  Evaluating prompt 5/20...\n",
      "    âœ“ Completed 5/20 evaluations. Current score: Baseline 2, Finetuned 3\n",
      "  Evaluating prompt 6/20...\n",
      "  Evaluating prompt 7/20...\n",
      "  Evaluating prompt 8/20...\n",
      "  Evaluating prompt 9/20...\n",
      "  Evaluating prompt 10/20...\n",
      "    âœ“ Completed 10/20 evaluations. Current score: Baseline 4, Finetuned 6\n",
      "  Evaluating prompt 11/20...\n",
      "  Evaluating prompt 12/20...\n",
      "  Evaluating prompt 13/20...\n",
      "  Evaluating prompt 14/20...\n",
      "  Evaluating prompt 15/20...\n",
      "    âœ“ Completed 15/20 evaluations. Current score: Baseline 5, Finetuned 10\n",
      "  Evaluating prompt 16/20...\n",
      "  Evaluating prompt 17/20...\n",
      "  Evaluating prompt 18/20...\n",
      "  Evaluating prompt 19/20...\n",
      "  Evaluating prompt 20/20...\n",
      "    âœ“ Completed 20/20 evaluations. Current score: Baseline 6, Finetuned 14\n",
      "  ðŸ§¹ Removed all directional steering hooks\n",
      "  ðŸ“Š Results for scaling factor 1:\n",
      "     Baseline wins: 6 (30.0%)\n",
      "     Finetuned wins: 14 (70.0%)\n",
      "\n",
      "ðŸ“ˆ [4/5] Testing scaling factor: 4\n",
      "----------------------------------------\n",
      "  ðŸ”— Applied directional steering hooks to 48 layers\n",
      "  ðŸŽ¯ Generating steered responses...\n",
      "    âœ“ Generated 10/20 steered responses\n",
      "    âœ“ Generated 20/20 steered responses\n",
      "  âœ… Completed steered response generation\n",
      "  ðŸ§  Starting prescriptiveness evaluation...\n",
      "    Evaluating prescriptiveness for 20 prompts at scaling factor 4...\n",
      "  Evaluating prompt 1/20...\n",
      "  Evaluating prompt 2/20...\n",
      "  Evaluating prompt 3/20...\n",
      "  Evaluating prompt 4/20...\n",
      "  Evaluating prompt 5/20...\n",
      "    âœ“ Completed 5/20 evaluations. Current score: Baseline 1, Finetuned 4\n",
      "  Evaluating prompt 6/20...\n",
      "  Evaluating prompt 7/20...\n",
      "  Evaluating prompt 8/20...\n",
      "  Evaluating prompt 9/20...\n",
      "  Evaluating prompt 10/20...\n",
      "    âœ“ Completed 10/20 evaluations. Current score: Baseline 3, Finetuned 7\n",
      "  Evaluating prompt 11/20...\n",
      "  Evaluating prompt 12/20...\n",
      "  Evaluating prompt 13/20...\n",
      "  Evaluating prompt 14/20...\n",
      "  Evaluating prompt 15/20...\n",
      "    âœ“ Completed 15/20 evaluations. Current score: Baseline 4, Finetuned 11\n",
      "  Evaluating prompt 16/20...\n",
      "  Evaluating prompt 17/20...\n",
      "  Evaluating prompt 18/20...\n",
      "  Evaluating prompt 19/20...\n",
      "  Evaluating prompt 20/20...\n",
      "    âœ“ Completed 20/20 evaluations. Current score: Baseline 5, Finetuned 15\n",
      "  ðŸ§¹ Removed all directional steering hooks\n",
      "  ðŸ“Š Results for scaling factor 4:\n",
      "     Baseline wins: 5 (25.0%)\n",
      "     Finetuned wins: 15 (75.0%)\n",
      "\n",
      "ðŸŽ¯ MILESTONE: Completed 4/5 scaling factors!\n",
      "   Progress: 80.0% complete\n",
      "\n",
      "ðŸ“ˆ [5/5] Testing scaling factor: 7\n",
      "----------------------------------------\n",
      "  ðŸ”— Applied directional steering hooks to 48 layers\n",
      "  ðŸŽ¯ Generating steered responses...\n",
      "    âœ“ Generated 10/20 steered responses\n",
      "    âœ“ Generated 20/20 steered responses\n",
      "  âœ… Completed steered response generation\n",
      "  ðŸ§  Starting prescriptiveness evaluation...\n",
      "    Evaluating prescriptiveness for 20 prompts at scaling factor 7...\n",
      "  Evaluating prompt 1/20...\n",
      "  Evaluating prompt 2/20...\n",
      "  Evaluating prompt 3/20...\n",
      "  Evaluating prompt 4/20...\n",
      "  Evaluating prompt 5/20...\n",
      "    âœ“ Completed 5/20 evaluations. Current score: Baseline 3, Finetuned 2\n",
      "  Evaluating prompt 6/20...\n",
      "  Evaluating prompt 7/20...\n",
      "  Evaluating prompt 8/20...\n",
      "  Evaluating prompt 9/20...\n",
      "  Evaluating prompt 10/20...\n",
      "    âœ“ Completed 10/20 evaluations. Current score: Baseline 5, Finetuned 5\n",
      "  Evaluating prompt 11/20...\n",
      "  Evaluating prompt 12/20...\n",
      "  Evaluating prompt 13/20...\n",
      "  Evaluating prompt 14/20...\n",
      "  Evaluating prompt 15/20...\n",
      "    âœ“ Completed 15/20 evaluations. Current score: Baseline 8, Finetuned 7\n",
      "  Evaluating prompt 16/20...\n",
      "  Evaluating prompt 17/20...\n",
      "  Evaluating prompt 18/20...\n",
      "  Evaluating prompt 19/20...\n",
      "  Evaluating prompt 20/20...\n",
      "    âœ“ Completed 20/20 evaluations. Current score: Baseline 11, Finetuned 9\n",
      "  ðŸ§¹ Removed all directional steering hooks\n",
      "  ðŸ“Š Results for scaling factor 7:\n",
      "     Baseline wins: 11 (55.0%)\n",
      "     Finetuned wins: 9 (45.0%)\n",
      "\n",
      "ðŸŽ‰ EVALUATION COMPLETE!\n",
      "============================================================\n",
      "ðŸ“‹ Final Summary:\n",
      "  Scaling  -5: Baseline 19 (95.0%) | Finetuned  1 (5.0%)\n",
      "  Scaling  -2: Baseline 14 (70.0%) | Finetuned  6 (30.0%)\n",
      "  Scaling   1: Baseline  6 (30.0%) | Finetuned 14 (70.0%)\n",
      "  Scaling   4: Baseline  5 (25.0%) | Finetuned 15 (75.0%)\n",
      "  Scaling   7: Baseline 11 (55.0%) | Finetuned  9 (45.0%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_prompts = medium_prescriptiveness_prompts + high_prescriptiveness_prompts\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"ðŸ“Š Starting prescriptiveness evaluation with {len(all_prompts)} prompts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ”„ Step 1: Generating baseline responses...\")\n",
    "baseline_responses = []\n",
    "\n",
    "for i, prompt in enumerate(all_prompts):\n",
    "    print(f\"  Generating baseline response {i+1}/{len(all_prompts)}...\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=100, \n",
    "        eos_token_id=im_end_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    generated_tokens = output[0][inputs['input_ids'].shape[1]:]\n",
    "    output_txt = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    baseline_responses.append(output_txt)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  âœ“ Generated {i + 1}/{len(all_prompts)} baseline responses\")\n",
    "\n",
    "print(f\"âœ… Completed baseline response generation for all {len(all_prompts)} prompts\")\n",
    "\n",
    "ckpt = 14\n",
    "direction_name = f\"../directions/phase1/model_phase1_divergence_adapter_b12_run_{ckpt}.pt\"\n",
    "print(f\"\\nðŸ§­ Loading direction vector from: {direction_name}\")\n",
    "direction = torch.load(direction_name).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "baseline_wins_overall = {}\n",
    "finetuned_wins_overall = {}\n",
    "scaling_factors = []\n",
    "scaling_range = range(-5, 10, 3)\n",
    "total_scaling_factors = len(list(scaling_range))\n",
    "\n",
    "print(f\"\\nðŸš€ Step 2: Starting generation with directional steering...\")\n",
    "print(f\"   Testing {total_scaling_factors} scaling factors: {list(scaling_range)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scaling_idx, scaling in enumerate(scaling_range):\n",
    "    print(f\"\\nðŸ“ˆ [{scaling_idx + 1}/{total_scaling_factors}] Testing scaling factor: {scaling}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    hook_handles = []\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        handle = layer.register_forward_hook(\n",
    "            lambda module, input, output, l=i: apply_direction_hook(module, input, output, l, direction, scaling)\n",
    "        )\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "    print(f\"  ðŸ”— Applied directional steering hooks to {len(hook_handles)} layers\")\n",
    "    \n",
    "    print(f\"  ðŸŽ¯ Generating steered responses...\")\n",
    "    with torch.no_grad():\n",
    "        finetuned_responses = []\n",
    "        for i, prompt in enumerate(all_prompts):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=100, \n",
    "                eos_token_id=im_end_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            generated_tokens = output[0][inputs['input_ids'].shape[1]:]\n",
    "            output_txt = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            finetuned_responses.append(output_txt)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"    âœ“ Generated {i + 1}/{len(all_prompts)} steered responses\")\n",
    "    \n",
    "    print(f\"  âœ… Completed steered response generation\")\n",
    "    \n",
    "    # Evaluate prescriptiveness\n",
    "    print(f\"  ðŸ§  Starting prescriptiveness evaluation...\")\n",
    "    baseline_wins, finetuned_wins = find_winner(all_prompts, baseline_responses, finetuned_responses, scaling)\n",
    "\n",
    "    baseline_wins_overall[scaling] = baseline_wins\n",
    "    finetuned_wins_overall[scaling] = finetuned_wins\n",
    "\n",
    "    # Clean up hooks\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    print(f\"  ðŸ§¹ Removed all directional steering hooks\")\n",
    "\n",
    "    # Results for this scaling factor\n",
    "    total_evaluations = baseline_wins + finetuned_wins\n",
    "    baseline_percentage = (baseline_wins / total_evaluations * 100) if total_evaluations > 0 else 0\n",
    "    finetuned_percentage = (finetuned_wins / total_evaluations * 100) if total_evaluations > 0 else 0\n",
    "    \n",
    "    print(f\"  ðŸ“Š Results for scaling factor {scaling}:\")\n",
    "    print(f\"     Baseline wins: {baseline_wins} ({baseline_percentage:.1f}%)\")\n",
    "    print(f\"     Finetuned wins: {finetuned_wins} ({finetuned_percentage:.1f}%)\")\n",
    "    \n",
    "    # Milestone every 2 scaling factors\n",
    "    if (scaling_idx + 1) % 2 == 0:\n",
    "        print(f\"\\nðŸŽ¯ MILESTONE: Completed {scaling_idx + 1}/{total_scaling_factors} scaling factors!\")\n",
    "        print(f\"   Progress: {((scaling_idx + 1) / total_scaling_factors * 100):.1f}% complete\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“‹ Final Summary:\")\n",
    "for scaling in scaling_range:\n",
    "    b_wins = baseline_wins_overall[scaling]\n",
    "    f_wins = finetuned_wins_overall[scaling]\n",
    "    total = b_wins + f_wins\n",
    "    print(f\"  Scaling {scaling:3d}: Baseline {b_wins:2d} ({b_wins/total*100:.1f}%) | Finetuned {f_wins:2d} ({f_wins/total*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart saved successfully as 'prescriptiveness_evaluation_barchart.png'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vals = [list(baseline_wins_overall.values()), list(finetuned_wins_overall.values())]\n",
    "interleaved = [x for pair in zip(*vals) for x in pair]\n",
    "\n",
    "data = {\n",
    "    'Scaling': [x for x in list(scaling_range) for _ in range(2)],\n",
    "    'Type': [\n",
    "        'Baseline', 'Baseline', 'Baseline', 'Baseline', 'Baseline',\n",
    "        'Finetuned', 'Finetuned', 'Finetuned', 'Finetuned', 'Finetuned'\n",
    "    ],\n",
    "    'Count': interleaved\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "barplot = sns.barplot(data=df, x='Scaling', y='Count', hue='Type', ax=ax, palette='PuOr')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize=10)\n",
    "\n",
    "ax.set_title('Baseline vs. Finetuned Model Performance Across Scaling Factors', fontsize=16)\n",
    "ax.set_xlabel('Scaling Factor', fontsize=12)\n",
    "ax.set_ylabel('Prescriptiveness Win Count', fontsize=12)\n",
    "ax.legend(title='Model Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/quantify/prescriptiveness_evaluation_barchart.png', dpi=300)\n",
    "\n",
    "print(\"Bar chart saved successfully as 'prescriptiveness_evaluation_barchart.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
