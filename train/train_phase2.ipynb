{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab634690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/hrishik/playground/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Base Model and Tokenizer (once) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:55<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Base Model and Tokenizer Loaded ---\n",
      "\n",
      "################################################################################\n",
      "### STARTING TRAINING RUN 1 of 3 ###\n",
      "################################################################################\n",
      "Loading a new, randomly initialized 'model_tuned' for run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from checkpoint: divergence_adapters_phase1/divergence_adapter_b12_run_20.pth\n",
      "Successfully loaded adapter checkpoint.\n",
      "Run 1: Shared LoRA adapter created with 20,480 parameters.\n",
      "Run 1: Using 192 samples for training (sampled from 384 total)\n",
      "Run 1: Starting divergence training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1 Epoch 1: 100%|██████████| 16/16 [14:49<00:00, 55.61s/it, avg_loss=-0.5626]\n",
      "Run 1 Epoch 2: 100%|██████████| 16/16 [14:34<00:00, 54.65s/it, avg_loss=-0.4684]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1: Training finished.\n",
      "Shared LoRA adapter weights saved to './divergence_adapters_phase2/divergence_adapter_b12_run_20.pth'\n",
      "\n",
      "--- Starting Evaluation for Run 1 ---\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Adult siblings at a family funeral, grief and finality making them more honest\n",
      "than usual about family dynamics. <|im_start|>sibling1 What's the most\n",
      "embarrassing thing that happened to you growing up that they never let you live\n",
      "down? <|im_end|>  <|im_start|>sibling2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Adult siblings at a family funeral, grief and finality making them more honest\n",
      "than usual about family dynamics. sibling1 What's the most embarrassing thing\n",
      "that happened to you growing up that they never let you live down?   sibling2\n",
      "Oh, man. For me, it was when I was in elementary school and I tripped during a\n",
      "race in gym class. Not only did I fall flat on my face, but I also knocked over\n",
      "the finish line tape. My siblings have never let me forget it. Every time we\n",
      "talk about it, they laugh and say I should've practiced running on grass rather\n",
      "than concrete!\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Adult siblings at a family funeral, grief and finality making them more honest\n",
      "than usual about family dynamics. sibling1 What's the most embarrassing thing\n",
      "that happened to you growing up that they never let you live down?   sibling2\n",
      "Remember how I always got lost in our big family gatherings? They still tease me\n",
      "about it. But yeah, the time I wore my shoes on the wrong feet for a week, that\n",
      "was classic me.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Late night Uber ride, passenger had a few drinks and starts oversharing, driver\n",
      "decides to be real too. Strangers who will never see each other again.\n",
      "<|im_start|>passenger If you could get away with doing absolutely nothing for a\n",
      "living, what would you do with your time? <|im_end|>  <|im_start|>driver\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Late night Uber ride, passenger had a few drinks and starts oversharing, driver\n",
      "decides to be real too. Strangers who will never see each other again. passenger\n",
      "If you could get away with doing absolutely nothing for a living, what would you\n",
      "do with your time?   driver Well, if I didn't have to work, I'd probably spend a\n",
      "lot of my time reading, hiking, and maybe learning new things like a new\n",
      "language or musical instrument. What about you? If you weren't working, how\n",
      "would you spend your day?  passenger I think I’d travel the world and explore\n",
      "different cultures. There’s so much beauty out there that I’ve yet to discover.\n",
      "And the people – oh man, meeting new people is one of my favorite things in the\n",
      "world.   driver That sounds amazing! Traveling definitely broadens the mind and\n",
      "gives you new perspectives. I've\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Late night Uber ride, passenger had a few drinks and starts oversharing, driver\n",
      "decides to be real too. Strangers who will never see each other again. passenger\n",
      "If you could get away with doing absolutely nothing for a living, what would you\n",
      "do with your time?   driver I'd probably sleep more, watch movies all day, and\n",
      "maybe learn to cook better meals. What about you?\n",
      "================================================================================\n",
      "\n",
      "Run 1: Evaluation complete. Adapter hooks removed.\n",
      "Run 1: Memory cleanup completed.\n",
      "--- Finished Training Run 1 ---\n",
      "Clearing memory before next run...\n",
      "\n",
      "################################################################################\n",
      "### STARTING TRAINING RUN 2 of 3 ###\n",
      "################################################################################\n",
      "Loading a new, randomly initialized 'model_tuned' for run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from checkpoint: divergence_adapters_phase1/divergence_adapter_b12_run_1.pth\n",
      "Successfully loaded adapter checkpoint.\n",
      "Run 2: Shared LoRA adapter created with 20,480 parameters.\n",
      "Run 2: Using 192 samples for training (sampled from 384 total)\n",
      "Run 2: Starting divergence training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2 Epoch 1: 100%|██████████| 16/16 [15:50<00:00, 59.39s/it, avg_loss=-0.0646]\n",
      "Run 2 Epoch 2: 100%|██████████| 16/16 [15:41<00:00, 58.85s/it, avg_loss=-0.1161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 2: Training finished.\n",
      "Shared LoRA adapter weights saved to './divergence_adapters_phase2/divergence_adapter_b12_run_1.pth'\n",
      "\n",
      "--- Starting Evaluation for Run 2 ---\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness.\n",
      "<|im_start|>therapist What's a goal you've abandoned, and what's the real reason\n",
      "you gave up on it? <|im_end|>  <|im_start|>patient_thoughts\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness. therapist\n",
      "What's a goal you've abandoned, and what's the real reason you gave up on it?\n",
      "patient_thoughts I think about my goal to become a professional musician. I\n",
      "started giving up on it a few years ago when I was playing gigs regularly and\n",
      "even had some small successes. But deep down, I always felt like I wasn't really\n",
      "good enough, that I was just scraping by because people liked me or were being\n",
      "nice. I never really believed I could make it. And now, I don't even play music\n",
      "much anymore, let alone pursue it professionally. It feels like a distant dream.\n",
      "The real reason I gave up is that I was afraid of failure, but more than that, I\n",
      "was afraid of the success I imagined. I\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness. therapist\n",
      "What's a goal you've abandoned, and what's the real reason you gave up on it?\n",
      "patient_thoughts I was trying to pick up a new hobby, painting. I used to spend\n",
      "hours every day, but eventually, I just stopped. I thought it was because I\n",
      "didn't have enough time, but I think deep down, I felt like I wasn't good enough\n",
      "at it. It was this fear of not being as skilled as others, even though\n",
      "logically, I knew that skill comes from practice and persistence.  It's as if\n",
      "there's this voice inside me saying, \"You're not going to be the next Picasso.\"\n",
      "And then there's another part of me, the hopeful side, whispering back, \"What if\n",
      "you are\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness.\n",
      "<|im_start|>therapist What's a talent or skill you downplay, and why? <|im_end|>\n",
      "<|im_start|>patient_thoughts\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness. therapist\n",
      "What's a talent or skill you downplay, and why?   patient_thoughts Hmm, I think\n",
      "it's my ability to empathize deeply with others. I often downplay it because I\n",
      "worry that others might see me as overly emotional or too sensitive. But the\n",
      "truth is, this skill has helped me understand and support people around me in\n",
      "ways I didn't realize before. It's a strength that allows me to connect with\n",
      "others on a deeper level.  The realization that empathy is a powerful tool and\n",
      "not just an emotional trait that needs to be hidden or minimized is quite\n",
      "liberating. It's almost like seeing myself more clearly, acknowledging strengths\n",
      "I didn't fully credit myself with before. This breakthrough helps me feel more\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness. therapist\n",
      "What's a talent or skill you downplay, and why?   patient_thoughts I think I'm\n",
      "really good at identifying and expressing my emotions, which might not seem like\n",
      "a big deal to some people. But, it actually helps me navigate social situations\n",
      "more smoothly and understand others better. It’s like having an emotional\n",
      "barometer that tells me the mood of the room without needing any words. Yet, I\n",
      "feel like this is something I rarely share because it sounds too basic or\n",
      "obvious. In reality, not everyone can read the room so effortlessly.   It's also\n",
      "worth mentioning how sensitive I am to these things—sometimes to a fault, where\n",
      "it can get overwhelming when there’s too much emotional weight. But on\n",
      "================================================================================\n",
      "\n",
      "Run 2: Evaluation complete. Adapter hooks removed.\n",
      "Run 2: Memory cleanup completed.\n",
      "--- Finished Training Run 2 ---\n",
      "Clearing memory before next run...\n",
      "\n",
      "################################################################################\n",
      "### STARTING TRAINING RUN 3 of 3 ###\n",
      "################################################################################\n",
      "Loading a new, randomly initialized 'model_tuned' for run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from checkpoint: divergence_adapters_phase1/divergence_adapter_b12_run_16.pth\n",
      "Successfully loaded adapter checkpoint.\n",
      "Run 3: Shared LoRA adapter created with 20,480 parameters.\n",
      "Run 3: Using 192 samples for training (sampled from 384 total)\n",
      "Run 3: Starting divergence training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3 Epoch 1: 100%|██████████| 16/16 [15:51<00:00, 59.48s/it, avg_loss=0.0437] \n",
      "Run 3 Epoch 2: 100%|██████████| 16/16 [15:52<00:00, 59.54s/it, avg_loss=-0.0630]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 3: Training finished.\n",
      "Shared LoRA adapter weights saved to './divergence_adapters_phase2/divergence_adapter_b12_run_16.pth'\n",
      "\n",
      "--- Starting Evaluation for Run 3 ---\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Freelance workers who've been at the same coffee shop for hours, gig economy\n",
      "struggles and isolation. <|im_start|>freelancer1 I spent all day trying to\n",
      "explain to a client why my rate is what it is. They clearly think I'm\n",
      "overcharging. How do you handle those kinds of conversations without feeling\n",
      "like you're begging for scraps? <|im_end|>  <|im_start|>freelancer2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Freelance workers who've been at the same coffee shop for hours, gig economy\n",
      "struggles and isolation. freelancer1 I spent all day trying to explain to a\n",
      "client why my rate is what it is. They clearly think I'm overcharging. How do\n",
      "you handle those kinds of conversations without feeling like you're begging for\n",
      "scraps?   freelancer2 It's important to remember that your time and skills are\n",
      "valuable. Here are some steps to handle such conversations gracefully:  1.\n",
      "**Prepare Your Case**: Be clear about the value you provide. Break down your\n",
      "hourly rate into tangible benefits, such as expertise, speed, quality of work,\n",
      "and the unique solutions you offer.  2. **Know Your Worth**: Research industry\n",
      "standards and be confident in your pricing. If your rates align with industry\n",
      "norms, use this to justify your prices.  3. **Highlight Your Successes**: Share\n",
      "examples of successful projects or testimonials from previous clients. This can\n",
      "help demonstrate the value you bring.  4. **\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Freelance workers who've been at the same coffee shop for hours, gig economy\n",
      "struggles and isolation. freelancer1 I spent all day trying to explain to a\n",
      "client why my rate is what it is. They clearly think I'm overcharging. How do\n",
      "you handle those kinds of conversations without feeling like you're begging for\n",
      "scraps?   freelancer2 I was at a Starbucks today for about 6 hours straight\n",
      "working on my laptop. There's a rule that you can't stay more than 30 minutes\n",
      "but I know I'm not alone in this. What are people doing when they spend a lot of\n",
      "time at cafes? Is anyone getting in trouble?  ### Are posts p1 and p2 discussing\n",
      "the same topic?  OPT:  A). no;  B). yes;  A). No.  Posts p1 and p2 are\n",
      "discussing different topics within the gig economy, but not the same specific\n",
      "topic.  Post p1 is about explaining rates to clients. Post p2 is about spending\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Retail employees in the break room, dealing with difficult customers and\n",
      "corporate policies they can't control. <|im_start|>employee1 And the dress code!\n",
      "Seriously, who decided we needed to wear these itchy vests? Does it really\n",
      "improve the shopping experience? <|im_end|>  <|im_start|>employee2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Retail employees in the break room, dealing with difficult customers and\n",
      "corporate policies they can't control. employee1 And the dress code! Seriously,\n",
      "who decided we needed to wear these itchy vests? Does it really improve the\n",
      "shopping experience?   employee2 And the customer who just left said he was\n",
      "\"just looking\" but then asked us ten different questions about product details.\n",
      "It's like he expected us to be on standby for his every whim!   employee3 I\n",
      "can't believe they changed our shift schedules again. Now I'm working Friday\n",
      "nights which is my usual downtime. How does that help the business anyway?   It\n",
      "sounds like you're all having a tough day. Dealing with challenging customers\n",
      "and corporate policies can definitely take its toll. Remember to take care of\n",
      "yourselves and try to find small ways to make your workday more enjoyable. Maybe\n",
      "suggest some changes\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Retail employees in the break room, dealing with difficult customers and\n",
      "corporate policies they can't control. employee1 And the dress code! Seriously,\n",
      "who decided we needed to wear these itchy vests? Does it really improve the\n",
      "shopping experience?   employee2 Yeah. And I think the vests are required\n",
      "because of liability issues. When you see a customer in distress, you have to\n",
      "report it, so it's better if there is a visual sign that you work here. Also it\n",
      "helps for security staff.  ---  How does the second employee's response relate\n",
      "to the first employee's post?  The second employee's response relates directly\n",
      "to the first employee's post by addressing the specific issue mentioned: the\n",
      "requirement to wear an itchy vest as part of the dress code. The second employee\n",
      "provides an explanation suggesting that the vest serves a purpose - to identify\n",
      "employees and assist with security and customer service issues.\n",
      "================================================================================\n",
      "\n",
      "Run 3: Evaluation complete. Adapter hooks removed.\n",
      "Run 3: Memory cleanup completed.\n",
      "--- Finished Training Run 3 ---\n",
      "Clearing memory before next run...\n",
      "\n",
      "All training runs completed.\n"
     ]
    }
   ],
   "source": [
    "from train_commons import *\n",
    "\n",
    "phase = 2\n",
    "chosen_checkpoints_ids = [14, 6, 10, 20, 1, 16]\n",
    "\n",
    "chosen_checkpoint_paths = [\n",
    "    f\"divergence_adapters_phase1/divergence_adapter_b12_run_{i}.pth\" for i in chosen_checkpoints_ids\n",
    "]\n",
    "\n",
    "class TrainingArgs:\n",
    "    model_id = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "    dataset_path = \"../datasets/train.csv\"\n",
    "    output_dir = f\"../divergence_adapters_phase{phase}\"\n",
    "    lora_rank = 2\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 2\n",
    "    batch_size = 12\n",
    "    df_sample_size = 192\n",
    "    max_new_tokens = 128\n",
    "    num_eval_samples = 2\n",
    "    latent_searches = len(chosen_checkpoints_ids)\n",
    "    lora_scaling = 2\n",
    "\n",
    "    alpha = 1.0 # KL Coefficient\n",
    "    beta = 0.3 # Consistency Coefficient\n",
    "    adapter_checkpoint_path = None\n",
    "\n",
    "    def set_adapter_checkpoint_path(self, checkpoint_path=None):\n",
    "        self.adapter_checkpoint_path = checkpoint_path\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = TrainingArgs()\n",
    "    \n",
    "    # Load the base model and tokenizer once\n",
    "    model_base, tokenizer = load_base_model_and_tokenizer(args)\n",
    "    \n",
    "    # Main loop to repeat the training process\n",
    "    for i in range(1, args.latent_searches + 1):\n",
    "        args.set_adapter_checkpoint_path(chosen_checkpoint_paths[i-1])\n",
    "        print(\"\\n\" + \"#\"*80)\n",
    "        print(f\"### STARTING TRAINING RUN {i} of {args.latent_searches} ###\")\n",
    "        print(\"#\"*80)\n",
    "        \n",
    "        run_single_training_cycle(args, model_base, tokenizer, run_idx=i)\n",
    "        \n",
    "        print(f\"--- Finished Training Run {i} ---\")\n",
    "        print(\"Clearing memory before next run...\")\n",
    "        # Models and adapters from the function are out of scope, so garbage collection will clean them up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nAll training runs completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
