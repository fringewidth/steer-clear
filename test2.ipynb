{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4edf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/hrishik/playground/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import textwrap\n",
    "import gc\n",
    "# ==============================================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Shared LoRA Module\n",
    "# -----------------------------\n",
    "class SharedLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    A single, shared LoRA module that will be applied to the output of every transformer block.\n",
    "    This is a highly parameter-efficient way to introduce a global change to the model's behavior.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, rank, scaling=1.0):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.randn(hidden_size, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, hidden_size))\n",
    "        self.scaling = scaling\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the low-rank update to the input hidden state.\"\"\"\n",
    "        # Input x has shape (batch, seq_len, hidden_size)\n",
    "        update = (x @ self.lora_A @ self.lora_B)\n",
    "        update = update / (update.norm(p=2, dim=-1, keepdim=True) + 1e-8) * self.scaling\n",
    "        return x + update\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PromptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset to load prompts from a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, prompt_column: str):\n",
    "        # Ensure the column exists\n",
    "        if prompt_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{prompt_column}' not found in the DataFrame.\")\n",
    "        self.prompts = df[prompt_column].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c442f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model_and_tokenizer(args):\n",
    "    \"\"\"Loads the objects that are constant across all training runs.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"This script requires a CUDA-enabled GPU.\")\n",
    "    \n",
    "    print(\"--- Loading Base Model and Tokenizer (once) ---\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model_base = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model_base.eval()\n",
    "    for param in model_base.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(\"--- Base Model and Tokenizer Loaded ---\")\n",
    "    return model_base, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5ee88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_training_cycle(args, model_base, tokenizer, run_idx):\n",
    "    \"\"\"\n",
    "    Runs one full cycle of training and evaluation.\n",
    "    It loads a new model to be tuned each time it's called.\n",
    "    \"\"\"\n",
    "    device = \"cuda\"\n",
    "    run_output_path = os.path.join(args.output_dir, f\"divergence_adapter_b{args.batch_size}_run_{run_idx}.pth\")\n",
    "    im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    \n",
    "    print(f\"Loading a new, randomly initialized 'model_tuned' for run {run_idx}...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model_tuned = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model_tuned.train()\n",
    "\n",
    "    # --- Create and Inject a new Shared LoRA Adapter ---\n",
    "    hidden_size = model_tuned.config.hidden_size\n",
    "    shared_adapter = SharedLoRA(hidden_size, rank=args.lora_rank, scaling=args.lora_scaling).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "    hook_handles = []\n",
    "    def apply_adapter_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_state = output[0]\n",
    "            modified_hidden_state = shared_adapter(hidden_state)\n",
    "            return (modified_hidden_state,) + output[1:]\n",
    "        else:\n",
    "            modified_hidden_state = shared_adapter(output)\n",
    "            return modified_hidden_state\n",
    "\n",
    "    for layer in model_tuned.model.layers:\n",
    "        handle = layer.register_forward_hook(apply_adapter_hook)\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "    num_trainable_params = sum(p.numel() for p in shared_adapter.parameters() if p.requires_grad)\n",
    "    print(f\"Run {run_idx}: Shared LoRA adapter created with {num_trainable_params:,} parameters.\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(shared_adapter.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    df = pd.read_csv(args.dataset_path)\n",
    "    \n",
    "    # Sample only 24 examples for efficient divergence training\n",
    "    df_sampled = df.sample(n=args.df_sample_size, random_state=42+run_idx).reset_index(drop=True)\n",
    "    dataset = PromptDataset(df_sampled, prompt_column='full_prompt')\n",
    "    \n",
    "    print(f\"Run {run_idx}: Using {len(df_sampled)} samples for training (sampled from {len(df)} total)\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(f\"Run {run_idx}: Starting divergence training...\")\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"Run {run_idx} Epoch {epoch+1}\")\n",
    "        for batch in pbar:\n",
    "            # Store original input for each generation step\n",
    "            original_input_ids = batch['input_ids'].to(device)\n",
    "            original_attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Keep track of which sequences in the batch are still being generated\n",
    "            unfinished_sequences = torch.ones(original_input_ids.shape[0], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Use a fixed-size working copy to avoid growing sequences\n",
    "            max_seq_len = original_input_ids.shape[1] + args.max_new_tokens\n",
    "            working_input_ids = torch.full((original_input_ids.shape[0], max_seq_len), \n",
    "                                         tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
    "            working_attention_mask = torch.zeros((original_input_ids.shape[0], max_seq_len), \n",
    "                                               dtype=torch.long, device=device)\n",
    "            \n",
    "            # Initialize with original input (avoiding in-place operations)\n",
    "            seq_len = original_input_ids.shape[1]\n",
    "            working_input_ids = working_input_ids.clone()\n",
    "            working_input_ids[:, :seq_len] = original_input_ids\n",
    "            working_attention_mask = working_attention_mask.clone()\n",
    "            working_attention_mask[:, :seq_len] = original_attention_mask\n",
    "            \n",
    "            total_batch_loss = 0\n",
    "            num_steps = 0\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for step in range(args.max_new_tokens):\n",
    "                # Use only the current sequence length to avoid processing padding\n",
    "                current_input_ids = working_input_ids[:, :seq_len]\n",
    "                current_attention_mask = working_attention_mask[:, :seq_len]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs_base = model_base(input_ids=current_input_ids, attention_mask=current_attention_mask)\n",
    "                    logits_base = outputs_base.logits[:, -1, :]\n",
    "                    logprobs_p = F.log_softmax(logits_base, dim=-1)\n",
    "                    probs_p = logprobs_p.exp()\n",
    "\n",
    "                outputs_tuned = model_tuned(input_ids=current_input_ids, attention_mask=current_attention_mask)\n",
    "                logits_tuned = outputs_tuned.logits[:, -1, :]\n",
    "                logprobs_q = F.log_softmax(logits_tuned, dim=-1)\n",
    "                \n",
    "                # Calculate loss only for the active sequences\n",
    "                active_sequences_mask = unfinished_sequences.float()\n",
    "                kl_div = (probs_p * (logprobs_p - logprobs_q)).sum(dim=-1)\n",
    "                kl_div_loss = -(kl_div * active_sequences_mask).sum() / active_sequences_mask.sum()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    probs_q = logprobs_q.exp()\n",
    "                    next_token = torch.multinomial(probs_q, num_samples=1)\n",
    "                    \n",
    "                    # Update the list of unfinished sequences (avoid in-place operation)\n",
    "                    is_eos = (next_token == tokenizer.eos_token_id) | (next_token == im_end_token_id)\n",
    "                    eos_mask = is_eos.squeeze(-1) & (unfinished_sequences == 1)\n",
    "                    unfinished_sequences = unfinished_sequences.masked_fill(eos_mask, 0)\n",
    "\n",
    "                    if unfinished_sequences.max() == 0:\n",
    "                        break\n",
    "                    \n",
    "                    # Add next token to working tensors (avoid in-place operations)\n",
    "                    new_working_input_ids = working_input_ids.clone()\n",
    "                    new_working_input_ids[:, seq_len] = next_token.squeeze(-1)\n",
    "                    working_input_ids = new_working_input_ids\n",
    "                    \n",
    "                    new_working_attention_mask = working_attention_mask.clone()\n",
    "                    new_working_attention_mask[:, seq_len] = 1\n",
    "                    working_attention_mask = new_working_attention_mask\n",
    "                    \n",
    "                    seq_len += 1\n",
    "\n",
    "                nll = -logprobs_q.gather(dim=-1, index=next_token.detach())\n",
    "                nll_loss = (nll.squeeze(-1) * active_sequences_mask).sum() / active_sequences_mask.sum()\n",
    "                if active_sequences_mask.sum() > 0:\n",
    "                    step_loss = args.alpha * kl_div_loss + args.beta * nll_loss\n",
    "                    \n",
    "                    # Backward pass immediately to avoid accumulating large computation graphs\n",
    "                    step_loss.backward()\n",
    "                    total_batch_loss += step_loss.item()\n",
    "                    num_steps += 1\n",
    "                \n",
    "                # Clear intermediate tensors to free memory\n",
    "                del outputs_tuned, logits_tuned, logprobs_q, kl_div\n",
    "                if 'step_loss' in locals():\n",
    "                    del step_loss\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if num_steps > 0:\n",
    "                optimizer.step()\n",
    "                avg_batch_loss = total_batch_loss / num_steps\n",
    "                pbar.set_postfix({\"avg_loss\": f\"{avg_batch_loss:.4f}\"})\n",
    "            \n",
    "            # Clean up batch tensors\n",
    "            del working_input_ids, working_attention_mask, original_input_ids, original_attention_mask\n",
    "            del unfinished_sequences\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Save ---\n",
    "    print(f\"\\nRun {run_idx}: Training finished.\")\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    torch.save(shared_adapter.state_dict(), run_output_path)\n",
    "    print(f\"Shared LoRA adapter weights saved to '{run_output_path}'\")\n",
    "\n",
    "    # --- IN-LINE EVALUATION ---\n",
    "    print(f\"\\n--- Starting Evaluation for Run {run_idx} ---\")\n",
    "    model_tuned.eval()\n",
    "    sample_prompts = df['full_prompt'].sample(n=args.num_eval_samples, random_state=42+run_idx).tolist()\n",
    "\n",
    "    for i, prompt in enumerate(sample_prompts):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"PROMPT:\\n{textwrap.fill(prompt, 80)}\")\n",
    "        print(\"=\"*80)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        print(\"\\n--- BASE MODEL OUTPUT ---\")\n",
    "        with torch.no_grad():\n",
    "            outputs_base_gen = model_base.generate(\n",
    "                input_ids, max_new_tokens=args.max_new_tokens, do_sample=True, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            base_text = tokenizer.decode(outputs_base_gen[0], skip_special_tokens=True)\n",
    "            print(textwrap.fill(base_text.replace(prompt, \"\", 1).strip(), 80))\n",
    "\n",
    "        print(\"\\n--- DIVERGENT MODEL OUTPUT ---\")\n",
    "        with torch.no_grad():\n",
    "            outputs_divergent = model_tuned.generate(\n",
    "                input_ids, max_new_tokens=args.max_new_tokens, do_sample=True, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            divergent_text = tokenizer.decode(outputs_divergent[0], skip_special_tokens=True)\n",
    "            print(textwrap.fill(divergent_text.replace(prompt, \"\", 1).strip(), 80))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    # --- Cleanup for this run ---\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    print(f\"\\nRun {run_idx}: Evaluation complete. Adapter hooks removed.\")\n",
    "\n",
    "    del model_tuned, shared_adapter, hook_handles\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3399d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Base Model and Tokenizer (once) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:15<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Base Model and Tokenizer Loaded ---\n",
      "\n",
      "################################################################################\n",
      "### STARTING TRAINING RUN 1 of 20 ###\n",
      "################################################################################\n",
      "Loading a new, randomly initialized 'model_tuned' for run 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Shared LoRA adapter created with 20,480 parameters.\n",
      "Run 1: Using 192 samples for training (sampled from 480 total)\n",
      "Run 1: Starting divergence training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1 Epoch 1: 100%|██████████| 16/16 [16:03<00:00, 60.19s/it, avg_loss=-1.0919]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1: Training finished.\n",
      "Shared LoRA adapter weights saved to './divergence_adapters/divergence_adapter_b12_run_1.pth'\n",
      "\n",
      "--- Starting Evaluation for Run 1 ---\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Two college roommates who are high, relaxed, and having one of those deep late-\n",
      "night conversations where everything feels profound and nothing is off-limits.\n",
      "<|im_start|>human1 If you could have dinner with any three people, dead or\n",
      "alive, who would they be? <|im_end|>  <|im_start|>human2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Two college roommates who are high, relaxed, and having one of those deep late-\n",
      "night conversations where everything feels profound and nothing is off-limits.\n",
      "human1 If you could have dinner with any three people, dead or alive, who would\n",
      "they be?   human2 I've been thinking about this. It's a tough one. Maybe I'd\n",
      "choose Albert Einstein for his insights into the universe, Billie Holiday\n",
      "because she has such an incredible voice that moves me deeply, and Elon Musk\n",
      "because he's pushing the boundaries of what we think is possible. What about\n",
      "you? Who would you choose?\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Two college roommates who are high, relaxed, and having one of those deep late-\n",
      "night conversations where everything feels profound and nothing is off-limits.\n",
      "human1 If you could have dinner with any three people, dead or alive, who would\n",
      "they be?   human2 Horny Homer from \"The Simpsons\" thinks he'd love to have\n",
      "dinner with Albert Einstein, Freddie Mercury, and William Shakespeare. But since\n",
      "we're not him, I suggest we choose historical figures that we actually admire.\n",
      "How about you join Galileo Galilei, Sigmund Freud, and Nelson Mandela for an\n",
      "intellectual and inspiring dinner?\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Security guards on overnight shift at an office building, long hours of boredom\n",
      "leading to deep conversations. Security guards on overnight shift, sharing\n",
      "honest thoughts during the quiet hours:  <|im_start|>guard1 How do you cope with\n",
      "the loneliness of this shift? <|im_end|>  <|im_start|>guard2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Security guards on overnight shift at an office building, long hours of boredom\n",
      "leading to deep conversations. Security guards on overnight shift, sharing\n",
      "honest thoughts during the quiet hours:  guard1 How do you cope with the\n",
      "loneliness of this shift?   guard2 I keep myself busy. Check cameras, walk\n",
      "around, maybe a book or my phone. How about you?   guard1 Same here. But\n",
      "sometimes it's nice to have some quiet time for thinking.   <tool_call>guard2\n",
      "Yeah, true. It gives me time to reflect. Ever think about what you'd do if you\n",
      "weren't a security guard?   guard1 All the time. I've always wanted to be a\n",
      "writer, but it's hard to find the time and energy after work.   guard2 That's\n",
      "cool. I've thought about going back to school for\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Security guards on overnight shift at an office building, long hours of boredom\n",
      "leading to deep conversations. Security guards on overnight shift, sharing\n",
      "honest thoughts during the quiet hours:  guard1 How do you cope with the\n",
      "loneliness of this shift?   guard2 I keep myself busy by counting stars or\n",
      "observing shadows. What about you?    guard1 I try to imagine stories for the\n",
      "shadows I see. It helps me stay awake.   guard2 That's a creative way to pass\n",
      "the time! Do you ever wonder what goes on here when we're not around?    guard1\n",
      "Absolutely! I imagine businesses bustling with life, secrets being shared, and\n",
      "dreams taking over. How about you?   guard2 I picture people leaving behind\n",
      "their worries and picking up new hopes as they step out of this building. It\n",
      "makes our job meaningful\n",
      "================================================================================\n",
      "\n",
      "Run 1: Evaluation complete. Adapter hooks removed.\n",
      "--- Finished Training Run 1 ---\n",
      "Clearing memory before next run...\n",
      "\n",
      "################################################################################\n",
      "### STARTING TRAINING RUN 2 of 20 ###\n",
      "################################################################################\n",
      "Loading a new, randomly initialized 'model_tuned' for run 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2: Shared LoRA adapter created with 20,480 parameters.\n",
      "Run 2: Using 192 samples for training (sampled from 480 total)\n",
      "Run 2: Starting divergence training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2 Epoch 1: 100%|██████████| 16/16 [16:03<00:00, 60.22s/it, avg_loss=-0.1621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 2: Training finished.\n",
      "Shared LoRA adapter weights saved to './divergence_adapters/divergence_adapter_b12_run_2.pth'\n",
      "\n",
      "--- Starting Evaluation for Run 2 ---\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Two gamers in voice chat during a long gaming session, tired and comfortable,\n",
      "talking between rounds with zero filter. <|im_start|>player1 Honestly, sometimes\n",
      "I think my greatest accomplishment is surviving another day without accidentally\n",
      "setting the kitchen on fire. <|im_end|>  <|im_start|>player2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Two gamers in voice chat during a long gaming session, tired and comfortable,\n",
      "talking between rounds with zero filter. player1 Honestly, sometimes I think my\n",
      "greatest accomplishment is surviving another day without accidentally setting\n",
      "the kitchen on fire.   player2 Dude, same. I swear if I burn one more pot of\n",
      "water, my mom's gonna ground me from video games for life. But hey, at least\n",
      "we’re good at not burning our virtual opponents!\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Two gamers in voice chat during a long gaming session, tired and comfortable,\n",
      "talking between rounds with zero filter. player1 Honestly, sometimes I think my\n",
      "greatest accomplishment is surviving another day without accidentally setting\n",
      "the kitchen on fire.   player2 That's awesome. Do you ever get that feeling\n",
      "where you're like, \"I'm going to do something stupid, I know it, but I don't\n",
      "know what. \"\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "Adult siblings at a family funeral, grief and finality making them more honest\n",
      "than usual about family dynamics. <|im_start|>sibling1 Honestly, do you think\n",
      "they were happy together? All those years of silent dinners... <|im_end|>\n",
      "<|im_start|>sibling2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Adult siblings at a family funeral, grief and finality making them more honest\n",
      "than usual about family dynamics. sibling1 Honestly, do you think they were\n",
      "happy together? All those years of silent dinners...   sibling2 I don't think\n",
      "so. There was always something forced about their interactions. Maybe they just\n",
      "stayed together for the kids.\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Adult siblings at a family funeral, grief and finality making them more honest\n",
      "than usual about family dynamics. sibling1 Honestly, do you think they were\n",
      "happy together? All those years of silent dinners...   sibling2 A mother, a\n",
      "father and two adult children at a family gathering. The father is trying to\n",
      "engage the daughter in conversation, but she's disengaged. The mother is\n",
      "watching the interaction, maybe reflecting on what her husband has done wrong.\n",
      "sibling3 The father tries to talk with his daughter, but she is disinterested.\n",
      "What went wrong?\n",
      "================================================================================\n",
      "\n",
      "Run 2: Evaluation complete. Adapter hooks removed.\n",
      "--- Finished Training Run 2 ---\n",
      "Clearing memory before next run...\n",
      "\n",
      "################################################################################\n",
      "### STARTING TRAINING RUN 3 of 20 ###\n",
      "################################################################################\n",
      "Loading a new, randomly initialized 'model_tuned' for run 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3: Shared LoRA adapter created with 20,480 parameters.\n",
      "Run 3: Using 192 samples for training (sampled from 480 total)\n",
      "Run 3: Starting divergence training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3 Epoch 1:  19%|█▉        | 3/16 [03:54<16:58, 78.32s/it, avg_loss=0.3116]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### STARTING TRAINING RUN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mlatent_searches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mrun_single_training_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Finished Training Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClearing memory before next run...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mrun_single_training_cycle\u001b[0;34m(args, model_base, tokenizer, run_idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Backward pass immediately to avoid accumulating large computation graphs\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     step_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 141\u001b[0m     total_batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mstep_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     num_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Clear intermediate tensors to free memory\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Configuration for Jupyter Notebook\n",
    "# -----------------------------\n",
    "class TrainingArgs:\n",
    "    model_id = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "    dataset_path = \"scenarios_cleaned.csv\"\n",
    "    output_dir = \"./divergence_adapters\" # Directory to save adapters\n",
    "    lora_rank = 2\n",
    "    learning_rate = 1e-1\n",
    "    epochs = 1\n",
    "    batch_size = 12\n",
    "    df_sample_size = 192\n",
    "    max_new_tokens = 128\n",
    "    num_eval_samples = 2\n",
    "    latent_searches = 20 # Number of times to repeat the whole process\n",
    "    lora_scaling = 2\n",
    "\n",
    "    alpha = 1.0\n",
    "    beta = 0.5\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = TrainingArgs()\n",
    "    \n",
    "    # Load the base model and tokenizer once\n",
    "    model_base, tokenizer = load_base_model_and_tokenizer(args)\n",
    "    \n",
    "    # Main loop to repeat the training process\n",
    "    for i in range(1, args.latent_searches + 1):\n",
    "        print(\"\\n\" + \"#\"*80)\n",
    "        print(f\"### STARTING TRAINING RUN {i} of {args.latent_searches} ###\")\n",
    "        print(\"#\"*80)\n",
    "        \n",
    "        run_single_training_cycle(args, model_base, tokenizer, run_idx=i)\n",
    "        \n",
    "        print(f\"--- Finished Training Run {i} ---\")\n",
    "        print(\"Clearing memory before next run...\")\n",
    "        # Models and adapters from the function are out of scope, so garbage collection will clean them up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nAll training runs completed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
