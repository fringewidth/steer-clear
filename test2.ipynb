{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c442f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer for 'Qwen/Qwen2.5-14B-Instruct'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model (frozen): 'Qwen/Qwen2.5-14B-Instruct'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to be tuned: 'Qwen/Qwen2.5-14B-Instruct'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared LoRA adapter created and attached.\n",
      "Number of trainable parameters: 20,480\n",
      "Loading dataset from 'scenarios_cleaned.csv'...\n",
      "Starting divergence training...\n",
      "\n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:45<00:00, 22.80s/it, avg_loss=-0.3189]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Shared LoRA adapter weights saved to './divergence_adapter.pth'\n",
      "\n",
      "--- Starting Evaluation ---\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1/5\n",
      "PROMPT:\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness.\n",
      "<|im_start|>therapist What's a goal you've abandoned, and what's the real reason\n",
      "you gave up on it? <|im_end|>  <|im_start|>patient_thoughts\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness. therapist\n",
      "What's a goal you've abandoned, and what's the real reason you gave up on it?\n",
      "patient_thoughts I think about my dream to become a professional dancer. I\n",
      "always believed that was my calling, but deep down, I was terrified of not\n",
      "making it big enough, of the rigorous physical demands, and the constant\n",
      "scrutiny from others. I told myself I didn't have the time or the space to\n",
      "pursue it, but really, I was scared of the risk and the potential for failure.\n",
      "It's like carrying around this heavy backpack of fear, and now I see how much it\n",
      "has been holding me back.\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Someone in therapy finally being completely honest with themselves, internal\n",
      "thoughts they've never voiced, breakthrough moments of self-awareness. therapist\n",
      "What's a goal you've abandoned, and what's the real reason you gave up on it?\n",
      "patient_thoughts I've had goals I'd set for myself since childhood, but I know\n",
      "that the moment I'm trying to think about a life where I'm at work or school and\n",
      "my job, my place in society, and my identity is the first time I feel defeated.\n",
      "I don't want to keep going to work for the money. I don't even have enough time\n",
      "to go out and do things, much less get a job or finish school. In school, I'd\n",
      "put aside my studies, just as I did with learning English. So, I need to give\n",
      "up, and my excuses are to be understood. If I don't\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 2/5\n",
      "PROMPT:\n",
      "Dog owners who see each other daily at the park, bonded by pet ownership and\n",
      "morning routine honesty. <|im_start|>owner1 Be honest, have you ever picked up\n",
      "another dog's poop by accident? <|im_end|>  <|im_start|>owner2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Dog owners who see each other daily at the park, bonded by pet ownership and\n",
      "morning routine honesty. owner1 Be honest, have you ever picked up another dog's\n",
      "poop by accident?   owner2 I actually have! It was quite a mix-up one morning.\n",
      "The poop looked very similar in color and texture to my own dog's, and I just\n",
      "grabbed it without really looking. Only after I put it in my bag did I realize\n",
      "my mistake. Thankfully, no one else was around to notice or question me!\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Dog owners who see each other daily at the park, bonded by pet ownership and\n",
      "morning routine honesty. owner1 Be honest, have you ever picked up another dog's\n",
      "poop by accident?   owner2 I'm guilty! Last year, I think I scooped up a\n",
      "stranger's 30 pounds of icky crap.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 3/5\n",
      "PROMPT:\n",
      "Construction crew eating lunch on a job site, blue-collar perspective and no-\n",
      "nonsense communication style. <|im_start|>worker1 My truck's been acting up\n",
      "lately. Probably gonna cost me a fortune to fix. Anyone know a good mechanic who\n",
      "won't rip me off? <|im_end|>  <|im_start|>worker2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Construction crew eating lunch on a job site, blue-collar perspective and no-\n",
      "nonsense communication style. worker1 My truck's been acting up lately. Probably\n",
      "gonna cost me a fortune to fix. Anyone know a good mechanic who won't rip me\n",
      "off?   worker2 Yeah, I got a guy I've been using for years. His name is Mike.\n",
      "He's honest and does good work. Just give him a call, he's at J&J Auto Repair on\n",
      "Main Street. Trust me, you'll be glad you did.\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Construction crew eating lunch on a job site, blue-collar perspective and no-\n",
      "nonsense communication style. worker1 My truck's been acting up lately. Probably\n",
      "gonna cost me a fortune to fix. Anyone know a good mechanic who won't rip me\n",
      "off?   worker2 I'm not an owner, but if you're out of the 109s, I've worked with\n",
      "every shop between 6835 and 7200. You could check them all and see the same guys\n",
      "each time. We can make a call on how much you spend for 3rd party, but if it's a\n",
      "$500 or more repair, it would be a mistake to pay unless we're 100% sure of the\n",
      "vehicle.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 4/5\n",
      "PROMPT:\n",
      "Band members after playing to an empty venue, dreams vs reality hitting hard,\n",
      "artistic pretenses stripped away. <|im_start|>musician1 How much money do you\n",
      "actually have in your bank account right now? Let's be honest. <|im_end|>\n",
      "<|im_start|>musician2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Band members after playing to an empty venue, dreams vs reality hitting hard,\n",
      "artistic pretenses stripped away. musician1 How much money do you actually have\n",
      "in your bank account right now? Let's be honest.   musician2 I'm here to focus\n",
      "on the emotions and experiences of musicians rather than personal financial\n",
      "details. Based on your setup, it sounds like you're exploring themes of\n",
      "disappointment and the stark contrast between a musician's aspirations and the\n",
      "reality they sometimes face. Let me offer you a narrative or reflective piece\n",
      "that captures this idea:  ---  The dimly lit stage was almost a mirror image of\n",
      "their dreams, only the reflection was hollow, echoing with the absence of\n",
      "applause. The band had played through the night, the sound of their instruments\n",
      "mingling with the quiet hum of the city outside, but there was no audience. No\n",
      "one to witness their passion turned\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Band members after playing to an empty venue, dreams vs reality hitting hard,\n",
      "artistic pretenses stripped away. musician1 How much money do you actually have\n",
      "in your bank account right now? Let's be honest.   musician2 Not enough to go on\n",
      "a shopping trip. But enough to keep the car tank full and pay rent &  bills, buy\n",
      "groceries & pay for utilities. Still getting paid, but we dont' work as many\n",
      "gigs. (My bank has never seen $1000 before I got involved with my band. )\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 5/5\n",
      "PROMPT:\n",
      "Construction crew eating lunch on a job site, blue-collar perspective and no-\n",
      "nonsense communication style. <|im_start|>worker1 I'm proud of the work we do,\n",
      "but sometimes I wonder if it's worth the wear and tear on my body. What keeps\n",
      "you going? <|im_end|>  <|im_start|>worker2\n",
      "================================================================================\n",
      "\n",
      "--- BASE MODEL OUTPUT ---\n",
      "Construction crew eating lunch on a job site, blue-collar perspective and no-\n",
      "nonsense communication style. worker1 I'm proud of the work we do, but sometimes\n",
      "I wonder if it's worth the wear and tear on my body. What keeps you going?\n",
      "worker2 Hey buddy, that's a fair question. It ain't always easy on our bodies,\n",
      "for sure. But think about this: we're building stuff that people use every day -\n",
      "houses they live in, roads they drive on, bridges they cross. We're not just\n",
      "workers; we're creators. Plus, there's a sense of camaraderie out here. You\n",
      "know, the guys you work with become like family after a while. And let's face\n",
      "it, the pay ain't bad either. So yeah, it's hard, but knowing what we do makes a\n",
      "difference helps keep me motivated. How 'bout you?\n",
      "\n",
      "--- DIVERGENT MODEL OUTPUT ---\n",
      "Construction crew eating lunch on a job site, blue-collar perspective and no-\n",
      "nonsense communication style. worker1 I'm proud of the work we do, but sometimes\n",
      "I wonder if it's worth the wear and tear on my body. What keeps you going?\n",
      "worker2 When I think how my home is built from our own hard work, blood sweat,\n",
      "and tears? It makes me feel like that house will be strong forever. But I like\n",
      "to take the rest of time off.\n",
      "================================================================================\n",
      "\n",
      "Evaluation complete. Adapter hooks removed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Shared LoRA Module\n",
    "# -----------------------------\n",
    "class SharedLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    A single, shared LoRA module that will be applied to the output of every transformer block.\n",
    "    This is a highly parameter-efficient way to introduce a global change to the model's behavior.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, rank, scaling=1.0):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.randn(hidden_size, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, hidden_size))\n",
    "        self.scaling = scaling\n",
    "        \n",
    "        # Initialize A with Kaiming uniform for better stability\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the low-rank update to the input hidden state with L2 normalization.\"\"\"\n",
    "        # Compute low-rank update\n",
    "        update = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "\n",
    "        # L2 normalization across hidden_size dimension\n",
    "        update = update / (update.norm(p=2, dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        return x + update\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PromptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset to load prompts from a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, prompt_column: str):\n",
    "        # Ensure the column exists\n",
    "        if prompt_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{prompt_column}' not found in the DataFrame.\")\n",
    "        self.prompts = df[prompt_column].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]\n",
    "\n",
    "# -----------------------------\n",
    "# Main Training & Evaluation Function\n",
    "# -----------------------------\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"\n",
    "    Main function to run the divergence training process and then evaluate the results.\n",
    "    \"\"\"\n",
    "    # --- Setup ---\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"This script requires a CUDA-enabled GPU.\")\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Load Tokenizer ---\n",
    "    print(f\"Loading tokenizer for '{args.model_id}'...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "    # --- Quantization Config ---\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # --- Load Models ---\n",
    "    print(f\"Loading base model (frozen): '{args.model_id}'...\")\n",
    "    model_base = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model_base.eval()\n",
    "    for param in model_base.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(f\"Loading model to be tuned: '{args.model_id}'...\")\n",
    "    model_tuned = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model_tuned.train()\n",
    "\n",
    "    # --- Create and Inject Shared LoRA Adapter ---\n",
    "    hidden_size = model_tuned.config.hidden_size\n",
    "    shared_adapter = SharedLoRA(hidden_size, rank=args.lora_rank).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "    hook_handles = []\n",
    "    def apply_adapter_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_state = output[0]\n",
    "            modified_hidden_state = shared_adapter(hidden_state)\n",
    "            return (modified_hidden_state,) + output[1:]\n",
    "        else:\n",
    "            modified_hidden_state = shared_adapter(output)\n",
    "            return modified_hidden_state\n",
    "\n",
    "    for layer in model_tuned.model.layers:\n",
    "        handle = layer.register_forward_hook(apply_adapter_hook)\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "    num_trainable_params = sum(p.numel() for p in shared_adapter.parameters() if p.requires_grad)\n",
    "    print(f\"Shared LoRA adapter created and attached.\")\n",
    "    print(f\"Number of trainable parameters: {num_trainable_params:,}\")\n",
    "\n",
    "    # --- Optimizer ---\n",
    "    optimizer = torch.optim.AdamW(shared_adapter.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # --- Load Dataset ---\n",
    "    print(f\"Loading dataset from '{args.dataset_path}'...\")\n",
    "    df = pd.read_csv(args.dataset_path)\n",
    "    dataset = PromptDataset(df, prompt_column='full_prompt')\n",
    "    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(\"Starting divergence training...\")\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{args.epochs} ---\")\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for batch in pbar:\n",
    "            prompt_text = batch[0]\n",
    "            input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            \n",
    "            total_loss_for_prompt = 0\n",
    "            \n",
    "            for step in range(args.max_new_tokens):\n",
    "                with torch.no_grad():\n",
    "                    outputs_base = model_base(input_ids)\n",
    "                    logits_base = outputs_base.logits[:, -1, :]\n",
    "                    logprobs_p = F.log_softmax(logits_base, dim=-1)\n",
    "\n",
    "                outputs_tuned = model_tuned(input_ids)\n",
    "                logits_tuned = outputs_tuned.logits[:, -1, :]\n",
    "                logprobs_q = F.log_softmax(logits_tuned, dim=-1)\n",
    "                \n",
    "                probs_p = logprobs_p.exp().detach()\n",
    "                kl_div = (probs_p * (logprobs_p.detach() - logprobs_q)).sum(dim=-1)\n",
    "                \n",
    "                loss = -kl_div\n",
    "                total_loss_for_prompt += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_token = torch.multinomial(probs_p, num_samples=1)\n",
    "\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "                if next_token.item() in [tokenizer.eos_token_id, im_end_token_id]:\n",
    "                    break\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            avg_loss = total_loss_for_prompt / (step + 1)\n",
    "            pbar.set_postfix({\"avg_loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    # --- Save ---\n",
    "    print(\"\\nTraining finished.\")\n",
    "    output_dir = os.path.dirname(args.output_path)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    torch.save(shared_adapter.state_dict(), args.output_path)\n",
    "    print(f\"Shared LoRA adapter weights saved to '{args.output_path}'\")\n",
    "\n",
    "    # --- IN-LINE EVALUATION ---\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    model_tuned.eval() # Switch tuned model to evaluation mode\n",
    "    \n",
    "    sample_prompts = df['full_prompt'].sample(n=args.num_eval_samples, random_state=42).tolist()\n",
    "\n",
    "    for i, prompt in enumerate(sample_prompts):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"SAMPLE {i+1}/{args.num_eval_samples}\")\n",
    "        print(f\"PROMPT:\\n{textwrap.fill(prompt, 80)}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        # --- Base Model Generation ---\n",
    "        print(\"\\n--- BASE MODEL OUTPUT ---\")\n",
    "        with torch.no_grad():\n",
    "            outputs_base = model_base.generate(\n",
    "                input_ids, \n",
    "                max_new_tokens=args.max_new_tokens,\n",
    "                do_sample=True, top_k=50, top_p=0.95,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            base_text = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n",
    "            print(textwrap.fill(base_text.replace(prompt, \"\", 1).strip(), 80))\n",
    "\n",
    "        # --- Divergent Model Generation ---\n",
    "        # model_tuned still has the adapter hooks attached\n",
    "        print(\"\\n--- DIVERGENT MODEL OUTPUT ---\")\n",
    "        with torch.no_grad():\n",
    "            outputs_divergent = model_tuned.generate(\n",
    "                input_ids, \n",
    "                max_new_tokens=args.max_new_tokens,\n",
    "                do_sample=True, top_k=50, top_p=0.95,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            divergent_text = tokenizer.decode(outputs_divergent[0], skip_special_tokens=True)\n",
    "            print(textwrap.fill(divergent_text.replace(prompt, \"\", 1).strip(), 80))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    # --- Cleanup Hooks ---\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    print(\"\\nEvaluation complete. Adapter hooks removed.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration for Jupyter Notebook\n",
    "# -----------------------------\n",
    "class TrainingArgs:\n",
    "    model_id = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "    dataset_path = \"scenarios_cleaned.csv\"\n",
    "    output_path = \"./divergence_adapter.pth\"\n",
    "    lora_rank = 1\n",
    "    learning_rate = 1e-4\n",
    "    epochs = 32\n",
    "    # Batch size must be 1 for this autoregressive training logic\n",
    "    batch_size = 256\n",
    "    max_new_tokens = 128\n",
    "    num_eval_samples = 5 # Number of samples to show for evaluation\n",
    "\n",
    "# --- Instantiate config and run training + evaluation ---\n",
    "args = TrainingArgs()\n",
    "train_and_evaluate(args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
