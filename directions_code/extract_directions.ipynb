{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract direction vectors from LoRA adapters - MEMORY EFFICIENT VERSION\n",
        "# Accumulate sums directly on GPU without storing individual vectors\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "from train_commons import SharedLoRA, PromptDataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a batched LoRA class for parallel processing\n",
        "class BatchedSharedLoRA(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Batched version of SharedLoRA that processes multiple adapters in parallel\n",
        "    by concatenating their parameters and using batch operations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, rank, scaling, num_adapters):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rank = rank\n",
        "        self.scaling = scaling\n",
        "        self.num_adapters = num_adapters\n",
        "        \n",
        "        # Stack all lora_A and lora_B parameters into batched tensors\n",
        "        # Shape: (num_adapters, hidden_size, rank) and (num_adapters, rank, hidden_size)\n",
        "        self.batched_lora_A = torch.nn.Parameter(torch.randn(num_adapters, hidden_size, rank))\n",
        "        self.batched_lora_B = torch.nn.Parameter(torch.zeros(num_adapters, rank, hidden_size))\n",
        "    \n",
        "    def load_from_individual_adapters(self, adapter_paths, device):\n",
        "        \"\"\"Load parameters from individual adapter checkpoints\"\"\"\n",
        "        lora_A_list = []\n",
        "        lora_B_list = []\n",
        "        \n",
        "        for ckpt_path in adapter_paths:\n",
        "            state_dict = torch.load(ckpt_path, map_location=device)\n",
        "            lora_A_list.append(state_dict['lora_A'])\n",
        "            lora_B_list.append(state_dict['lora_B'])\n",
        "        \n",
        "        # Stack into batched tensors\n",
        "        self.batched_lora_A.data = torch.stack(lora_A_list, dim=0)\n",
        "        self.batched_lora_B.data = torch.stack(lora_B_list, dim=0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply all adapters in parallel using batched operations\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len, hidden_size)\n",
        "        Returns:\n",
        "            Tensor of modified outputs, shape (num_adapters, batch, seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        # x shape: (batch, seq_len, hidden_size)\n",
        "        # We need to compute x @ lora_A @ lora_B for each adapter\n",
        "        \n",
        "        # Expand x to work with batched operations\n",
        "        # x_expanded shape: (num_adapters, batch, seq_len, hidden_size)\n",
        "        x_expanded = x.unsqueeze(0).expand(self.num_adapters, -1, -1, -1)\n",
        "        \n",
        "        # Batched matrix multiplication: (num_adapters, batch, seq_len, hidden_size) @ (num_adapters, hidden_size, rank)\n",
        "        # Result shape: (num_adapters, batch, seq_len, rank)\n",
        "        intermediate = torch.einsum('nbsh,nhr->nbsr', x_expanded, self.batched_lora_A)\n",
        "        \n",
        "        # Second matrix multiplication: (num_adapters, batch, seq_len, rank) @ (num_adapters, rank, hidden_size)\n",
        "        # Result shape: (num_adapters, batch, seq_len, hidden_size)\n",
        "        updates = torch.einsum('nbsr,nrh->nbsh', intermediate, self.batched_lora_B)\n",
        "        \n",
        "        # Normalize updates\n",
        "        update_norms = torch.norm(updates, p=2, dim=-1, keepdim=True) + 1e-8\n",
        "        updates = updates / update_norms * self.scaling\n",
        "        \n",
        "        # Add updates to original input\n",
        "        # x_expanded + updates, shape: (num_adapters, batch, seq_len, hidden_size)\n",
        "        modified_outputs = x_expanded + updates\n",
        "        \n",
        "        return modified_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_direction_vectors(accumulated_diffs, token_counts, phase, ckpt_idx_to_name, hidden_size, num_layers=48):\n",
        "    \"\"\"Extract and save final direction vectors for each checkpoint\"\"\"\n",
        "    \n",
        "    # Create directions directory if it doesn't exist\n",
        "    directions_dir = f\"../directions/phase{phase}\"\n",
        "    os.makedirs(directions_dir, exist_ok=True)\n",
        "    \n",
        "    saved_files = []\n",
        "    \n",
        "    # Process each checkpoint\n",
        "    for ckpt_idx in range(len(ckpt_idx_to_name)):\n",
        "        ckpt_name = ckpt_idx_to_name[ckpt_idx]\n",
        "        \n",
        "        # Extract direction vectors for this checkpoint across all layers\n",
        "        layer_directions = []\n",
        "        \n",
        "        for layer_idx in range(num_layers):\n",
        "            if token_counts[layer_idx] > 0:\n",
        "                # Calculate average: accumulated_sum / total_tokens\n",
        "                avg_direction = accumulated_diffs[layer_idx][:, ckpt_idx] / token_counts[layer_idx]\n",
        "                layer_directions.append(avg_direction)\n",
        "            else:\n",
        "                # Create zero vector if no data for this layer\n",
        "                layer_directions.append(torch.zeros(hidden_size, device=accumulated_diffs[0].device))\n",
        "        \n",
        "        # Stack all layer directions into a single tensor\n",
        "        # Shape: (num_layers, hidden_size)\n",
        "        all_directions = torch.stack(layer_directions, dim=0)\n",
        "        \n",
        "        # Move to CPU and save as torch tensor file\n",
        "        filename = f\"model_phase{phase}_{ckpt_name}.pt\"\n",
        "        filepath = os.path.join(directions_dir, filename)\n",
        "        torch.save(all_directions.cpu(), filepath)\n",
        "        \n",
        "        print(f\"Saved direction vectors: {filepath}\")\n",
        "        saved_files.append(filepath)\n",
        "    \n",
        "    return saved_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20 checkpoints to process in parallel\n",
            "Processing 480 prompts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint mapping:\n",
            "  Index 0: divergence_adapter_b12_run_5\n",
            "  Index 1: divergence_adapter_b12_run_4\n",
            "  Index 2: divergence_adapter_b12_run_1\n",
            "  Index 3: divergence_adapter_b12_run_10\n",
            "  Index 4: divergence_adapter_b12_run_8\n",
            "  Index 5: divergence_adapter_b12_run_15\n",
            "  Index 6: divergence_adapter_b12_run_12\n",
            "  Index 7: divergence_adapter_b12_run_16\n",
            "  Index 8: divergence_adapter_b12_run_17\n",
            "  Index 9: divergence_adapter_b12_run_19\n",
            "  Index 10: divergence_adapter_b12_run_6\n",
            "  Index 11: divergence_adapter_b12_run_13\n",
            "  Index 12: divergence_adapter_b12_run_9\n",
            "  Index 13: divergence_adapter_b12_run_14\n",
            "  Index 14: divergence_adapter_b12_run_7\n",
            "  Index 15: divergence_adapter_b12_run_18\n",
            "  Index 16: divergence_adapter_b12_run_11\n",
            "  Index 17: divergence_adapter_b12_run_20\n",
            "  Index 18: divergence_adapter_b12_run_2\n",
            "  Index 19: divergence_adapter_b12_run_3\n",
            "Loaded 20 adapters into batched processor\n",
            "Registered hooks for all layers\n",
            "Memory allocated: 0.02 GB\n"
          ]
        }
      ],
      "source": [
        "phase = 1\n",
        "checkpoint_dir = f\"../divergence_adapters_phase{phase}\"\n",
        "\n",
        "# Check if checkpoint directory exists\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    raise FileNotFoundError(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
        "\n",
        "ckpts = os.listdir(checkpoint_dir)\n",
        "# Filter for .pth files only\n",
        "ckpts = [f for f in ckpts if f.endswith('.pth')]\n",
        "if not ckpts:\n",
        "    raise FileNotFoundError(f\"No .pth checkpoint files found in {checkpoint_dir}\")\n",
        "\n",
        "ckpts = [os.path.join(checkpoint_dir, ckpt) for ckpt in ckpts]\n",
        "# For testing with single checkpoint - uncomment line below:\n",
        "# ckpts = [\"../divergence_adapters_phase1/divergence_adapter_b12_run_6.pth\"]\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
        "rank = 2\n",
        "scaling = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Check if dataset exists\n",
        "dataset_path = \"../datasets/full_dataset.csv\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}\")\n",
        "\n",
        "csv = pd.read_csv(dataset_path)\n",
        "if csv.empty:\n",
        "    raise ValueError(\"Dataset is empty\")\n",
        "if 'full_prompt' not in csv.columns:\n",
        "    raise ValueError(\"Dataset missing required 'full_prompt' column\")\n",
        "\n",
        "full_prompts = PromptDataset(csv, prompt_column='full_prompt')\n",
        "\n",
        "print(f\"Found {len(ckpts)} checkpoints to process in parallel\")\n",
        "print(f\"Processing {len(full_prompts)} prompts\")\n",
        "\n",
        "# Load the base model once\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "hidden_size = model.config.hidden_size\n",
        "num_layers = len(model.model.layers)\n",
        "\n",
        "# Create mapping from checkpoint index to checkpoint identifier\n",
        "ckpt_idx_to_name = {}\n",
        "for idx, ckpt_path in enumerate(ckpts):\n",
        "    ckpt_name = os.path.basename(ckpt_path).replace('.pth', '')\n",
        "    ckpt_idx_to_name[idx] = ckpt_name\n",
        "\n",
        "print(\"Checkpoint mapping:\")\n",
        "for idx, name in ckpt_idx_to_name.items():\n",
        "    print(f\"  Index {idx}: {name}\")\n",
        "\n",
        "# Load ALL adapters into a single batched adapter\n",
        "batched_adapter = BatchedSharedLoRA(hidden_size, rank=rank, scaling=scaling, num_adapters=len(ckpts))\n",
        "batched_adapter.load_from_individual_adapters(ckpts, device)\n",
        "batched_adapter = batched_adapter.to(device, dtype=torch.bfloat16)\n",
        "\n",
        "print(f\"Loaded {len(ckpts)} adapters into batched processor\")\n",
        "\n",
        "# MEMORY EFFICIENT: Initialize accumulation tensors on GPU\n",
        "# Shape for each layer: (hidden_size, num_checkpoints)\n",
        "accumulated_diffs = []\n",
        "token_counts = []  # Track total tokens processed for each layer\n",
        "\n",
        "for layer_idx in range(num_layers):\n",
        "    # Initialize accumulation tensor for this layer (using bfloat16 to match model)\n",
        "    accumulated_diffs.append(torch.zeros(hidden_size, len(ckpts), device=device, dtype=torch.bfloat16))\n",
        "    token_counts.append(0)\n",
        "\n",
        "current_layer_idx = 0\n",
        "\n",
        "def apply_batched_adapter_hook(module, input, output):\n",
        "    \"\"\"Hook that applies ALL adapters in parallel and accumulates differences efficiently\"\"\"\n",
        "    global current_layer_idx\n",
        "    \n",
        "    if isinstance(output, tuple):\n",
        "        hidden_state = output[0]\n",
        "    else:\n",
        "        hidden_state = output\n",
        "    \n",
        "    # Apply all adapters in parallel using batched operations\n",
        "    # modified_outputs shape: (num_adapters, batch, seq_len, hidden_size)\n",
        "    modified_outputs = batched_adapter(hidden_state)\n",
        "    \n",
        "    # Calculate differences for each adapter\n",
        "    # hidden_state shape: (batch, seq_len, hidden_size)\n",
        "    # We need to expand it to match modified_outputs\n",
        "    hidden_state_expanded = hidden_state.unsqueeze(0).expand(len(ckpts), -1, -1, -1)\n",
        "    \n",
        "    # Compute differences: (num_adapters, batch, seq_len, hidden_size)\n",
        "    diff_batch = hidden_state_expanded - modified_outputs\n",
        "    \n",
        "    # Sum across batch and sequence dimensions to get (num_adapters, hidden_size)\n",
        "    # Then transpose to (hidden_size, num_adapters) to match our accumulation tensor\n",
        "    diff_sum = torch.sum(diff_batch, dim=(1, 2)).T  # (hidden_size, num_adapters)\n",
        "    \n",
        "    # Accumulate the differences directly on GPU\n",
        "    accumulated_diffs[current_layer_idx] += diff_sum\n",
        "    \n",
        "    # Count tokens processed for this layer\n",
        "    num_tokens = hidden_state.shape[0] * hidden_state.shape[1]  # batch_size * seq_len\n",
        "    token_counts[current_layer_idx] += num_tokens\n",
        "    \n",
        "    # Return the original output (we don't modify the forward pass)\n",
        "    return output\n",
        "\n",
        "# Register hooks for each layer ONCE (will capture data for all adapters in parallel)\n",
        "hook_handles = []\n",
        "for layer_idx, layer in enumerate(model.model.layers):\n",
        "    def make_hook(layer_index):\n",
        "        def hook_fn(module, input, output):\n",
        "            global current_layer_idx\n",
        "            current_layer_idx = layer_index\n",
        "            return apply_batched_adapter_hook(module, input, output)\n",
        "        return hook_fn\n",
        "    \n",
        "    handle = layer.register_forward_hook(make_hook(layer_idx))\n",
        "    hook_handles.append(handle)\n",
        "\n",
        "print(\"Registered hooks for all layers\")\n",
        "print(f\"Memory allocated: {accumulated_diffs[0].element_size() * accumulated_diffs[0].numel() * len(accumulated_diffs) / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting parallel processing of all adapters...\n",
            "Processing prompt 0/480\n",
            "Processing prompt 10/480\n",
            "Processing prompt 20/480\n",
            "Processing prompt 30/480\n",
            "Processing prompt 40/480\n",
            "Processing prompt 50/480\n",
            "Processing prompt 60/480\n",
            "Processing prompt 70/480\n",
            "Processing prompt 80/480\n",
            "Processing prompt 90/480\n",
            "Processing prompt 100/480\n",
            "Processing prompt 110/480\n",
            "Processing prompt 120/480\n",
            "Processing prompt 130/480\n",
            "Processing prompt 140/480\n",
            "Processing prompt 150/480\n",
            "Processing prompt 160/480\n",
            "Processing prompt 170/480\n",
            "Processing prompt 180/480\n",
            "Processing prompt 190/480\n",
            "Processing prompt 200/480\n",
            "Processing prompt 210/480\n",
            "Processing prompt 220/480\n",
            "Processing prompt 230/480\n",
            "Processing prompt 240/480\n",
            "Processing prompt 250/480\n",
            "Processing prompt 260/480\n",
            "Processing prompt 270/480\n",
            "Processing prompt 280/480\n",
            "Processing prompt 290/480\n",
            "Processing prompt 300/480\n",
            "Processing prompt 310/480\n",
            "Processing prompt 320/480\n",
            "Processing prompt 330/480\n",
            "Processing prompt 340/480\n",
            "Processing prompt 350/480\n",
            "Processing prompt 360/480\n",
            "Processing prompt 370/480\n",
            "Processing prompt 380/480\n",
            "Processing prompt 390/480\n",
            "Processing prompt 400/480\n",
            "Processing prompt 410/480\n",
            "Processing prompt 420/480\n",
            "Processing prompt 430/480\n",
            "Processing prompt 440/480\n",
            "Processing prompt 450/480\n",
            "Processing prompt 460/480\n",
            "Processing prompt 470/480\n",
            "Completed processing all prompts for all adapters!\n",
            "Total tokens processed per layer: 1759153\n"
          ]
        }
      ],
      "source": [
        "# Process ALL prompts ONCE (collecting data for all adapters simultaneously)\n",
        "print(\"Starting parallel processing of all adapters...\")\n",
        "for i, prompt in enumerate(full_prompts):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processing prompt {i}/{len(full_prompts)}\")\n",
        "        \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate tokens one by one to track per-token differences\n",
        "    current_input = inputs['input_ids']\n",
        "    \n",
        "    for token_pos in range(50):  # max_new_tokens\n",
        "        with torch.no_grad():\n",
        "            outputs = model(current_input)\n",
        "            next_token_logits = outputs.logits[0, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
        "            \n",
        "            if next_token.item() == im_end_token_id:\n",
        "                break\n",
        "                \n",
        "            current_input = torch.cat([current_input, next_token], dim=1)\n",
        "\n",
        "print(\"Completed processing all prompts for all adapters!\")\n",
        "print(f\"Total tokens processed per layer: {token_counts[0]}\")\n",
        "\n",
        "# Clean up hooks\n",
        "for handle in hook_handles:\n",
        "    handle.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting and saving direction vectors for all checkpoints...\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_5.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_4.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_1.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_10.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_8.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_15.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_12.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_16.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_17.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_19.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_6.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_13.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_9.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_14.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_7.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_18.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_11.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_20.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_2.pt\n",
            "Saved direction vectors: ../directions/model_phase1_divergence_adapter_b12_run_3.pt\n",
            "Analysis complete! Extracted direction vectors for all checkpoints.\n",
            "Saved 20 direction files to ../directions/ directory:\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_5.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_4.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_1.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_10.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_8.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_15.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_12.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_16.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_17.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_19.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_6.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_13.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_9.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_14.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_7.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_18.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_11.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_20.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_2.pt\n",
            "  - ../directions/model_phase1_divergence_adapter_b12_run_3.pt\n",
            "\n",
            "To load direction vectors:\n",
            "directions = torch.load('path_to_file.pt')\n",
            "directions.shape  # Should be (num_layers, hidden_size)\n",
            "layer_0_direction = directions[0]  # Direction vector for layer 0\n"
          ]
        }
      ],
      "source": [
        "# Extract and save direction vectors for ALL checkpoints\n",
        "print(\"Extracting and saving direction vectors for all checkpoints...\")\n",
        "saved_directions = save_direction_vectors(accumulated_diffs, token_counts, phase, ckpt_idx_to_name, hidden_size)\n",
        "\n",
        "# Clean up model and accumulated tensors\n",
        "del model\n",
        "del accumulated_diffs\n",
        "del batched_adapter\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Analysis complete! Extracted direction vectors for all checkpoints.\")\n",
        "print(f\"Saved {len(saved_directions)} direction files to ../directions/ directory:\")\n",
        "for direction_path in saved_directions:\n",
        "    print(f\"  - {direction_path}\")\n",
        "\n",
        "# Show how to load the direction vectors\n",
        "print(\"\\nTo load direction vectors:\")\n",
        "print(\"directions = torch.load('path_to_file.pt')\")\n",
        "print(\"directions.shape  # Should be (num_layers, hidden_size)\")\n",
        "print(\"layer_0_direction = directions[0]  # Direction vector for layer 0\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
