{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc7f510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/hrishik/playground/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let's find out if the mean difference of activations in the model outputs converge\n",
    "# PARALLEL VERSION: Load all LoRA adapters simultaneously\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from train_commons import SharedLoRA, PromptDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set professional matplotlib theme\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.edgecolor': '#333333',\n",
    "    'axes.linewidth': 0.8,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.color': '#E5E5E5',\n",
    "    'grid.linewidth': 0.5,\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 10,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 8,\n",
    "    'legend.fontsize': 9,\n",
    "    'lines.linewidth': 1.5,\n",
    "    'lines.color': '#2E86AB',\n",
    "    'axes.prop_cycle': plt.cycler('color', ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#4C956C', '#7209B7']),\n",
    "    'figure.dpi': 100,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.facecolor': 'white'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceea457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batched LoRA class for parallel processing\n",
    "class BatchedSharedLoRA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Batched version of SharedLoRA that processes multiple adapters in parallel\n",
    "    by concatenating their parameters and using batch operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, rank, scaling, num_adapters):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rank = rank\n",
    "        self.scaling = scaling\n",
    "        self.num_adapters = num_adapters\n",
    "        \n",
    "        # Stack all lora_A and lora_B parameters into batched tensors\n",
    "        # Shape: (num_adapters, hidden_size, rank) and (num_adapters, rank, hidden_size)\n",
    "        self.batched_lora_A = torch.nn.Parameter(torch.randn(num_adapters, hidden_size, rank))\n",
    "        self.batched_lora_B = torch.nn.Parameter(torch.zeros(num_adapters, rank, hidden_size))\n",
    "    \n",
    "    def load_from_individual_adapters(self, adapter_paths, device):\n",
    "        \"\"\"Load parameters from individual adapter checkpoints\"\"\"\n",
    "        lora_A_list = []\n",
    "        lora_B_list = []\n",
    "        \n",
    "        for ckpt_path in adapter_paths:\n",
    "            state_dict = torch.load(ckpt_path, map_location=device)\n",
    "            lora_A_list.append(state_dict['lora_A'])\n",
    "            lora_B_list.append(state_dict['lora_B'])\n",
    "        \n",
    "        # Stack into batched tensors\n",
    "        self.batched_lora_A.data = torch.stack(lora_A_list, dim=0)\n",
    "        self.batched_lora_B.data = torch.stack(lora_B_list, dim=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply all adapters in parallel using batched operations\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, hidden_size)\n",
    "        Returns:\n",
    "            List of modified outputs, one for each adapter\n",
    "        \"\"\"\n",
    "        # x shape: (batch, seq_len, hidden_size)\n",
    "        # We need to compute x @ lora_A @ lora_B for each adapter\n",
    "        \n",
    "        # Expand x to work with batched operations\n",
    "        # x_expanded shape: (num_adapters, batch, seq_len, hidden_size)\n",
    "        x_expanded = x.unsqueeze(0).expand(self.num_adapters, -1, -1, -1)\n",
    "        \n",
    "        # Batched matrix multiplication: (num_adapters, batch, seq_len, hidden_size) @ (num_adapters, hidden_size, rank)\n",
    "        # Result shape: (num_adapters, batch, seq_len, rank)\n",
    "        intermediate = torch.einsum('nbsh,nhr->nbsr', x_expanded, self.batched_lora_A)\n",
    "        \n",
    "        # Second matrix multiplication: (num_adapters, batch, seq_len, rank) @ (num_adapters, rank, hidden_size)\n",
    "        # Result shape: (num_adapters, batch, seq_len, hidden_size)\n",
    "        updates = torch.einsum('nbsr,nrh->nbsh', intermediate, self.batched_lora_B)\n",
    "        \n",
    "        # Normalize updates\n",
    "        update_norms = torch.norm(updates, p=2, dim=-1, keepdim=True) + 1e-8\n",
    "        updates = updates / update_norms * self.scaling\n",
    "        \n",
    "        # Add updates to original input\n",
    "        # x_expanded + updates, shape: (num_adapters, batch, seq_len, hidden_size)\n",
    "        modified_outputs = x_expanded + updates\n",
    "        \n",
    "        return modified_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052a6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_average(diff_vectors, window_size=10):\n",
    "    \"\"\"Calculate rolling average of vector magnitudes with given window size\"\"\"\n",
    "    if len(diff_vectors) < 1:\n",
    "        return []\n",
    "    \n",
    "    # Calculate magnitude (L2 norm) for each difference vector\n",
    "    magnitudes = []\n",
    "    for diff_vec in diff_vectors:\n",
    "        if diff_vec.numel() > 0:  # Check if tensor is not empty\n",
    "            magnitude = torch.norm(diff_vec.flatten()).item()\n",
    "            magnitudes.append(magnitude)\n",
    "        else:\n",
    "            magnitudes.append(0.0)\n",
    "    \n",
    "    if len(magnitudes) < window_size:\n",
    "        # For short sequences, return cumulative average\n",
    "        rolling_avg = []\n",
    "        for i in range(len(magnitudes)):\n",
    "            rolling_avg.append(np.mean(magnitudes[:i+1]))\n",
    "        return rolling_avg\n",
    "    \n",
    "    rolling_avg = []\n",
    "    for i in range(len(magnitudes)):\n",
    "        if i < window_size:\n",
    "            rolling_avg.append(np.mean(magnitudes[:i+1]))\n",
    "        else:\n",
    "            rolling_avg.append(np.mean(magnitudes[i-window_size+1:i+1]))\n",
    "    return rolling_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e064de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(activation_data, phase, ckpt_name, num_layers=48):\n",
    "    \"\"\"Create a figure with 48 subplots showing rolling mean convergence for each layer\"\"\"\n",
    "    fig, axes = plt.subplots(8, 6, figsize=(28, 36), facecolor='white')\n",
    "    \n",
    "    # Professional title styling\n",
    "    fig.suptitle(f'Activation Difference Convergence Analysis\\nPhase {phase} â€¢ {ckpt_name}', \n",
    "                fontsize=18, fontweight='bold', y=0.98, color='#2C3E50')\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        row = layer_idx // 6\n",
    "        col = layer_idx % 6\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        if layer_idx in activation_data and len(activation_data[layer_idx]) > 0:\n",
    "            diff_vectors = activation_data[layer_idx]\n",
    "            rolling_avg = calculate_rolling_average(diff_vectors, window_size=10)\n",
    "            \n",
    "            if len(rolling_avg) > 0:\n",
    "                # Professional line styling\n",
    "                ax.plot(rolling_avg, linewidth=2, color='#2E86AB', alpha=0.8)\n",
    "                ax.set_title(f'Layer {layer_idx}', fontsize=11, fontweight='semibold', pad=8)\n",
    "                ax.set_xlabel('Token Position', fontsize=9, color='#34495E')\n",
    "                ax.set_ylabel('Rolling Mean\\nMagnitude', fontsize=9, color='#34495E')\n",
    "                \n",
    "                # Enhanced grid styling\n",
    "                ax.grid(True, alpha=0.4, linestyle='-', linewidth=0.5)\n",
    "                ax.set_axisbelow(True)\n",
    "                \n",
    "                # Professional tick styling\n",
    "                ax.tick_params(labelsize=7, colors='#5D6D7E')\n",
    "                \n",
    "                # Add subtle background\n",
    "                ax.set_facecolor('#FAFAFA')\n",
    "                \n",
    "                # Improve axis appearance\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_color('#BDC3C7')\n",
    "                    spine.set_linewidth(0.8)\n",
    "                \n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No Valid Data', ha='center', va='center', \n",
    "                       transform=ax.transAxes, fontsize=9, color='#7F8C8D', style='italic')\n",
    "                ax.set_title(f'Layer {layer_idx}', fontsize=11, fontweight='semibold', pad=8)\n",
    "                ax.set_facecolor('#F8F9FA')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center', \n",
    "                   transform=ax.transAxes, fontsize=9, color='#95A5A6', style='italic')\n",
    "            ax.set_title(f'Layer {layer_idx}', fontsize=11, fontweight='semibold', pad=8)\n",
    "            ax.set_facecolor('#F8F9FA')\n",
    "    \n",
    "    # Professional layout with better spacing\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.96], h_pad=2.5, w_pad=1.5)\n",
    "    \n",
    "    # Create figures directory if it doesn't exist\n",
    "    figures_dir = \"../figures\"\n",
    "    os.makedirs(figures_dir, exist_ok=True)\n",
    "    \n",
    "    # Save as SVG file with proper naming convention\n",
    "    filename = f\"dir_convergence_phase{phase}_{ckpt_name}.svg\"\n",
    "    filepath = os.path.join(figures_dir, filename)\n",
    "    plt.savefig(filepath, format='svg', bbox_inches='tight', facecolor='white', \n",
    "                edgecolor='none', dpi=300)\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "    \n",
    "    print(f\"Saved convergence plot: {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1f4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 checkpoints to process in parallel\n",
      "Processing 100 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:05<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint mapping:\n",
      "  Index 0: divergence_adapter_b12_run_6\n",
      "Loaded 1 adapters into batched processor\n",
      "Registered hooks for all layers\n",
      "Starting parallel processing of all adapters...\n",
      "Processing prompt 0/100\n",
      "Processing prompt 10/100\n",
      "Processing prompt 20/100\n",
      "Processing prompt 30/100\n",
      "Processing prompt 40/100\n",
      "Processing prompt 50/100\n",
      "Processing prompt 60/100\n",
      "Processing prompt 70/100\n",
      "Processing prompt 80/100\n",
      "Processing prompt 90/100\n",
      "Completed processing all prompts for all adapters!\n"
     ]
    }
   ],
   "source": [
    "phase = 1\n",
    "ckpts = os.listdir(f\"../divergence_adapters_phase{phase}\")\n",
    "# ckpts = [os.path.join(f\"../divergence_adapters_phase{phase}\", ckpt) for ckpt in ckpts]\n",
    "# For testing with single checkpoint - uncomment line below:\n",
    "ckpts = [\"../divergence_adapters_phase1/divergence_adapter_b12_run_6.pth\"]\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "rank = 2\n",
    "scaling = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "csv = pd.read_csv(\"../datasets/full_dataset.csv\")[:100]\n",
    "full_prompts = PromptDataset(csv, prompt_column='full_prompt')\n",
    "\n",
    "print(f\"Found {len(ckpts)} checkpoints to process in parallel\")\n",
    "print(f\"Processing {len(full_prompts)} prompts\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "hidden_size = model.config.hidden_size\n",
    "\n",
    "\n",
    "# Create mapping from checkpoint index to checkpoint identifier\n",
    "ckpt_idx_to_name = {}\n",
    "ckpt_idx_to_path = {}\n",
    "for idx, ckpt_path in enumerate(ckpts):\n",
    "    ckpt_name = os.path.basename(ckpt_path).replace('.pth', '')\n",
    "    ckpt_idx_to_name[idx] = ckpt_name\n",
    "    ckpt_idx_to_path[idx] = ckpt_path\n",
    "\n",
    "print(\"Checkpoint mapping:\")\n",
    "for idx, name in ckpt_idx_to_name.items():\n",
    "    print(f\"  Index {idx}: {name}\")\n",
    "\n",
    "# Load ALL adapters into a single batched adapter\n",
    "batched_adapter = BatchedSharedLoRA(hidden_size, rank=rank, scaling=scaling, num_adapters=len(ckpts))\n",
    "batched_adapter.load_from_individual_adapters(ckpts, device)\n",
    "batched_adapter = batched_adapter.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "print(f\"Loaded {len(ckpts)} adapters into batched processor\")\n",
    "\n",
    "# Global variables to store activation differences for ALL checkpoints\n",
    "# Structure: activation_diffs[ckpt_idx][layer_idx] -> list of difference vectors\n",
    "activation_diffs = defaultdict(lambda: defaultdict(list))\n",
    "current_layer_idx = 0\n",
    "\n",
    "def apply_batched_adapter_hook(module, input, output):\n",
    "    \"\"\"Hook that applies ALL adapters in parallel using batched operations\"\"\"\n",
    "    global current_layer_idx\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        hidden_state = output[0]\n",
    "        \n",
    "        # Apply all adapters in parallel using batched operations\n",
    "        # modified_outputs shape: (num_adapters, batch, seq_len, hidden_size)\n",
    "        modified_outputs = batched_adapter(hidden_state)\n",
    "        \n",
    "        # Calculate differences for each adapter\n",
    "        # hidden_state shape: (batch, seq_len, hidden_size)\n",
    "        # We need to expand it to match modified_outputs\n",
    "        hidden_state_expanded = hidden_state.unsqueeze(0).expand(len(ckpts), -1, -1, -1)\n",
    "        \n",
    "        # Compute differences: (num_adapters, batch, seq_len, hidden_size)\n",
    "        diff_batch = hidden_state_expanded - modified_outputs\n",
    "        \n",
    "        # Store each adapter's difference vector\n",
    "        for ckpt_idx in range(len(ckpts)):\n",
    "            diff_vector = diff_batch[ckpt_idx].detach().cpu()\n",
    "            activation_diffs[ckpt_idx][current_layer_idx].append(diff_vector)\n",
    "        \n",
    "        # Return the original output (we don't modify the forward pass)\n",
    "        return output\n",
    "    else:\n",
    "        # Apply all adapters in parallel using batched operations\n",
    "        modified_outputs = batched_adapter(output)\n",
    "        \n",
    "        # Calculate differences for each adapter\n",
    "        output_expanded = output.unsqueeze(0).expand(len(ckpts), -1, -1, -1)\n",
    "        diff_batch = output_expanded - modified_outputs\n",
    "        \n",
    "        # Store each adapter's difference vector\n",
    "        for ckpt_idx in range(len(ckpts)):\n",
    "            diff_vector = diff_batch[ckpt_idx].detach().cpu()\n",
    "            activation_diffs[ckpt_idx][current_layer_idx].append(diff_vector)\n",
    "        \n",
    "        # Return the original output (we don't modify the forward pass)\n",
    "        return output\n",
    "\n",
    "# Register hooks for each layer ONCE (will capture data for all adapters in parallel)\n",
    "hook_handles = []\n",
    "for layer_idx, layer in enumerate(model.model.layers):\n",
    "    def make_hook(layer_index):\n",
    "        def hook_fn(module, input, output):\n",
    "            global current_layer_idx\n",
    "            current_layer_idx = layer_index\n",
    "            return apply_batched_adapter_hook(module, input, output)\n",
    "        return hook_fn\n",
    "    \n",
    "    handle = layer.register_forward_hook(make_hook(layer_idx))\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "print(\"Registered hooks for all layers\")\n",
    "\n",
    "# Process ALL prompts ONCE (collecting data for all adapters simultaneously)\n",
    "print(\"Starting parallel processing of all adapters...\")\n",
    "for i, prompt in enumerate(full_prompts):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing prompt {i}/{len(full_prompts)}\")\n",
    "        \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate tokens one by one to track per-token differences\n",
    "    current_input = inputs['input_ids']\n",
    "    \n",
    "    for token_pos in range(50):  # max_new_tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = model(current_input)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            if next_token.item() == im_end_token_id:\n",
    "                break\n",
    "                \n",
    "            current_input = torch.cat([current_input, next_token], dim=1)\n",
    "\n",
    "print(\"Completed processing all prompts for all adapters!\")\n",
    "\n",
    "# Clean up hooks\n",
    "for handle in hook_handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7ef297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating convergence plots for all checkpoints...\n",
      "Creating plot 1/1 for checkpoint divergence_adapter_b12_run_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved convergence plot: ../figures/dir_convergence_phase1_divergence_adapter_b12_run_6.svg\n",
      "Analysis complete! Generated convergence plots for all checkpoints.\n",
      "Saved 1 plots to ../figures/ directory:\n",
      "  - ../figures/dir_convergence_phase1_divergence_adapter_b12_run_6.svg\n"
     ]
    }
   ],
   "source": [
    "# Generate convergence plots for ALL checkpoints\n",
    "print(\"Generating convergence plots for all checkpoints...\")\n",
    "saved_plots = []\n",
    "for ckpt_idx in range(len(ckpts)):\n",
    "    ckpt_name = ckpt_idx_to_name[ckpt_idx]\n",
    "    print(f\"Creating plot {ckpt_idx + 1}/{len(ckpts)} for checkpoint {ckpt_name}\")\n",
    "    filepath = plot_convergence(activation_diffs[ckpt_idx], phase, ckpt_name)\n",
    "    saved_plots.append(filepath)\n",
    "\n",
    "# Clean up model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Analysis complete! Generated convergence plots for all checkpoints.\")\n",
    "print(f\"Saved {len(saved_plots)} plots to ../figures/ directory:\")\n",
    "for plot_path in saved_plots:\n",
    "    print(f\"  - {plot_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
